<!DOCTYPE html><html lang="zh-CN" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no"><title>【机器学习】入门ML？跟着做这几个实验足矣！（一） | 泷汐の精神时光屋</title><meta name="keywords" content="机器学习"><meta name="author" content="TideDra,GearyZhang@outlook.com"><meta name="copyright" content="TideDra"><meta name="format-detection" content="telephone=no"><meta name="theme-color" content="#ffffff"><meta name="description" content="前言 本系列记录《机器学习》课的实验部分。实验所有资料在此下载：百度网盘 提取码：8848 线性回归——音乐年代预测 目录 一. 概述：音乐年代预测与回归问题 二. 数据集的加载与展示 三. 特征筛选与预处理 四. 线性回归 五. 多项式回归 六. 岭回归 一. 概述：音乐年代预测与回归问题 1. 音乐年代预测问题  根据一首音乐的音频特征，推测这首音乐的发行年份。  问题的输入：音乐的特征 问题">
<meta property="og:type" content="article">
<meta property="og:title" content="【机器学习】入门ML？跟着做这几个实验足矣！（一）">
<meta property="og:url" content="https://tidedra.top/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/ML-exp1/index.html">
<meta property="og:site_name" content="泷汐の精神时光屋">
<meta property="og:description" content="前言 本系列记录《机器学习》课的实验部分。实验所有资料在此下载：百度网盘 提取码：8848 线性回归——音乐年代预测 目录 一. 概述：音乐年代预测与回归问题 二. 数据集的加载与展示 三. 特征筛选与预处理 四. 线性回归 五. 多项式回归 六. 岭回归 一. 概述：音乐年代预测与回归问题 1. 音乐年代预测问题  根据一首音乐的音频特征，推测这首音乐的发行年份。  问题的输入：音乐的特征 问题">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://img.zsaqwq.com/images/2022/03/28/ML_exp1_cover.jpg">
<meta property="article:published_time" content="2022-03-28T07:48:41.000Z">
<meta property="article:modified_time" content="2022-03-28T12:47:30.584Z">
<meta property="article:author" content="TideDra">
<meta property="article:tag" content="机器学习">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://img.zsaqwq.com/images/2022/03/28/ML_exp1_cover.jpg"><link rel="shortcut icon" href="/img/business-man.png"><link rel="canonical" href="https://tidedra.top/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/ML-exp1/"><link rel="preconnect" href="//cdn.jsdelivr.net"/><link rel="preconnect" href="//busuanzi.ibruce.info"/><meta name="google-site-verification" content="ev-WLSJNE26pCrlvY2Pg6U4IEX9_RPuCr1PkKJ5oewo"/><link rel="stylesheet" href="/css/index.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@6/css/all.min.css" media="print" onload="this.media='all'"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/node-snackbar/dist/snackbar.min.css" media="print" onload="this.media='all'"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fancyapps/ui/dist/fancybox.css" media="print" onload="this.media='all'"><script>const GLOBAL_CONFIG = { 
  root: '/',
  algolia: {"appId":"S2H9LOYZXK","apiKey":"afe57ba6d261bf3d2627d49ad352458c","indexName":"MyBlog","hits":{"per_page":6},"languages":{"input_placeholder":"搜索文章","hits_empty":"找不到您查询的内容：${query}","hits_stats":"找到 ${hits} 条结果，用时 ${time} 毫秒"}},
  localSearch: undefined,
  translate: undefined,
  noticeOutdate: undefined,
  highlight: {"plugin":"highlighjs","highlightCopy":true,"highlightLang":true,"highlightHeightLimit":false},
  copy: {
    success: '复制成功',
    error: '复制错误',
    noSupport: '浏览器不支持'
  },
  relativeDate: {
    homepage: false,
    post: false
  },
  runtime: '',
  date_suffix: {
    just: '刚刚',
    min: '分钟前',
    hour: '小时前',
    day: '天前',
    month: '个月前'
  },
  copyright: undefined,
  lightbox: 'fancybox',
  Snackbar: {"chs_to_cht":"你已切换为繁体","cht_to_chs":"你已切换为简体","day_to_night":"你已切换为深色模式","night_to_day":"你已切换为浅色模式","bgLight":"#49b1f5","bgDark":"#1f1f1f","position":"bottom-center"},
  source: {
    justifiedGallery: {
      js: 'https://cdn.jsdelivr.net/npm/flickr-justified-gallery@2/dist/fjGallery.min.js',
      css: 'https://cdn.jsdelivr.net/npm/flickr-justified-gallery@2/dist/fjGallery.min.css'
    }
  },
  isPhotoFigcaption: false,
  islazyload: false,
  isAnchor: false
}</script><script id="config-diff">var GLOBAL_CONFIG_SITE = {
  title: '【机器学习】入门ML？跟着做这几个实验足矣！（一）',
  isPost: true,
  isHome: false,
  isHighlightShrink: false,
  isToc: true,
  postUpdate: '2022-03-28 20:47:30'
}</script><noscript><style type="text/css">
  #nav {
    opacity: 1
  }
  .justified-gallery img {
    opacity: 1
  }

  #recent-posts time,
  #post-meta time {
    display: inline !important
  }
</style></noscript><script>(win=>{
    win.saveToLocal = {
      set: function setWithExpiry(key, value, ttl) {
        if (ttl === 0) return
        const now = new Date()
        const expiryDay = ttl * 86400000
        const item = {
          value: value,
          expiry: now.getTime() + expiryDay,
        }
        localStorage.setItem(key, JSON.stringify(item))
      },

      get: function getWithExpiry(key) {
        const itemStr = localStorage.getItem(key)

        if (!itemStr) {
          return undefined
        }
        const item = JSON.parse(itemStr)
        const now = new Date()

        if (now.getTime() > item.expiry) {
          localStorage.removeItem(key)
          return undefined
        }
        return item.value
      }
    }
  
    win.getScript = url => new Promise((resolve, reject) => {
      const script = document.createElement('script')
      script.src = url
      script.async = true
      script.onerror = reject
      script.onload = script.onreadystatechange = function() {
        const loadState = this.readyState
        if (loadState && loadState !== 'loaded' && loadState !== 'complete') return
        script.onload = script.onreadystatechange = null
        resolve()
      }
      document.head.appendChild(script)
    })
  
      win.activateDarkMode = function () {
        document.documentElement.setAttribute('data-theme', 'dark')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#0d0d0d')
        }
      }
      win.activateLightMode = function () {
        document.documentElement.setAttribute('data-theme', 'light')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#ffffff')
        }
      }
      const t = saveToLocal.get('theme')
    
          if (t === 'dark') activateDarkMode()
          else if (t === 'light') activateLightMode()
        
      const asideStatus = saveToLocal.get('aside-status')
      if (asideStatus !== undefined) {
        if (asideStatus === 'hide') {
          document.documentElement.classList.add('hide-aside')
        } else {
          document.documentElement.classList.remove('hide-aside')
        }
      }
    
    const detectApple = () => {
      if(/iPad|iPhone|iPod|Macintosh/.test(navigator.userAgent)){
        document.documentElement.classList.add('apple')
      }
    }
    detectApple()
    })(window)</script><meta name="generator" content="Hexo 6.1.0"></head><body><div id="loading-box"><div class="loading-left-bg"></div><div class="loading-right-bg"></div><div class="spinner-box"><div class="configure-border-1"><div class="configure-core"></div></div><div class="configure-border-2"><div class="configure-core"></div></div><div class="loading-word">加载中...</div></div></div><div id="web_bg"></div><div id="sidebar"><div id="menu-mask"></div><div id="sidebar-menus"><div class="avatar-img is-center"><img src="https://img.zsaqwq.com/images/2022/03/26/girl.jpg" onerror="onerror=null;src='/img/friend_404.gif'" alt="avatar"/></div><div class="site-data is-center"><div class="data-item"><a href="/archives/"><div class="headline">文章</div><div class="length-num">9</div></a></div><div class="data-item"><a href="/tags/"><div class="headline">标签</div><div class="length-num">11</div></a></div><div class="data-item"><a href="/categories/"><div class="headline">分类</div><div class="length-num">6</div></a></div></div><hr/><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> 首页</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> 归档</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> 标签</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> 分类</span></a></div><div class="menus_item"><a class="site-page" href="/link"><i class="fa-fw fa-solid fa-link"></i><span> 链接</span></a></div><div class="menus_item"><a class="site-page" href="/artitalk/"><i class="fa-fw fa-solid fa-comments"></i><span> 说说</span></a></div></div></div></div><div class="post" id="body-wrap"><header class="post-bg" id="page-header" style="background-image: url('https://img.zsaqwq.com/images/2022/03/28/ML_exp1_cover.jpg')"><nav id="nav"><span id="blog_name"><a id="site-name" href="/">泷汐の精神时光屋</a></span><div id="menus"><div id="search-button"><a class="site-page social-icon search"><i class="fas fa-search fa-fw"></i><span> 搜索</span></a></div><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> 首页</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> 归档</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> 标签</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> 分类</span></a></div><div class="menus_item"><a class="site-page" href="/link"><i class="fa-fw fa-solid fa-link"></i><span> 链接</span></a></div><div class="menus_item"><a class="site-page" href="/artitalk/"><i class="fa-fw fa-solid fa-comments"></i><span> 说说</span></a></div></div><div id="toggle-menu"><a class="site-page"><i class="fas fa-bars fa-fw"></i></a></div></div></nav><div id="post-info"><h1 class="post-title">【机器学习】入门ML？跟着做这几个实验足矣！（一）</h1><div id="post-meta"><div class="meta-firstline"><span class="post-meta-date"><i class="far fa-calendar-alt fa-fw post-meta-icon"></i><span class="post-meta-label">发表于</span><time class="post-meta-date-created" datetime="2022-03-28T07:48:41.000Z" title="发表于 2022-03-28 15:48:41">2022-03-28</time><span class="post-meta-separator">|</span><i class="fas fa-history fa-fw post-meta-icon"></i><span class="post-meta-label">更新于</span><time class="post-meta-date-updated" datetime="2022-03-28T12:47:30.584Z" title="更新于 2022-03-28 20:47:30">2022-03-28</time></span><span class="post-meta-categories"><span class="post-meta-separator">|</span><i class="fas fa-inbox fa-fw post-meta-icon"></i><a class="post-meta-categories" href="/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/">机器学习</a></span></div><div class="meta-secondline"><span class="post-meta-separator">|</span><span class="post-meta-pv-cv" id="" data-flag-title="【机器学习】入门ML？跟着做这几个实验足矣！（一）"><i class="far fa-eye fa-fw post-meta-icon"></i><span class="post-meta-label">阅读量:</span><span id="busuanzi_value_page_pv"></span></span></div></div></div></header><main class="layout" id="content-inner"><div id="post"><article class="post-content" id="article-container"><h1>前言</h1>
<p>本系列记录《机器学习》课的实验部分。实验所有资料在此下载：<a target="_blank" rel="noopener" href="https://pan.baidu.com/s/1yGS52ZBfd-oCePur7u42uQ?pwd=8848">百度网盘</a> 提取码：8848</p>
<h1>线性回归——音乐年代预测</h1>
<h2 id="目录">目录</h2>
<p>一. 概述：音乐年代预测与回归问题<br>
二. 数据集的加载与展示<br>
三. 特征筛选与预处理<br>
四. 线性回归<br>
五. 多项式回归<br>
六. 岭回归</p>
<h1>一. 概述：音乐年代预测与回归问题</h1>
<h2 id="1-音乐年代预测问题">1. 音乐年代预测问题</h2>
<ul>
<li>根据一首音乐的音频特征，推测这首音乐的发行年份。
<ul>
<li>问题的输入：音乐的特征</li>
<li>问题的输出：发行年份（是一个实数）</li>
</ul>
</li>
<li>音乐年代预测问题是一个比较典型的<em><strong>回归问题</strong></em>。</li>
</ul>
<h2 id="2-回归问题（regression）">2. 回归问题（regression）</h2>
<ul>
<li>和分类问题一样，回归问题也是以数据的特征为输入，以数据的标签为输出。</li>
<li>回归与分类的区别
<ul>
<li>分类模型的输出是离散的、孤立的类别，比如鸢尾花的种类。</li>
<li>回归模型的输出是连续的<strong>数值</strong>，比如这里的年份可以近似看作是连续的。</li>
<li>分类问题是提供定性的输出，回归问题是提供定量的输出。</li>
</ul>
</li>
<li>生活中的回归问题
<ul>
<li>房屋价格预测
<ul>
<li>输入一个房屋的地理位置、面积等特征</li>
<li>输出这个房屋的价格</li>
</ul>
</li>
<li>气温预测
<ul>
<li>输入某个时刻</li>
<li>输出该时刻的气温</li>
</ul>
</li>
</ul>
</li>
</ul>
<p>接下来，实验正式开始。</p>
<ul>
<li>我们需要首先安装<code>PrettyTable</code>模组，它可以帮我们打印好看的表格（如果已安装可忽略）。</li>
<li>接着我们导入本课程代码所需要的<code>python</code>库（不能在后文的代码中自行import）。</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 安装PrettyTable模组</span></span><br><span class="line">! pip install PrettyTable</span><br></pre></td></tr></table></figure>
<blockquote>
<p>Requirement already satisfied: PrettyTable in d:\application\anaconda\lib\site-packages (3.0.0)<br>
Requirement already satisfied: wcwidth in d:\application\anaconda\lib\site-packages (from PrettyTable) (0.2.5)</p>
</blockquote>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 导入Python库</span></span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">import</span> seaborn <span class="keyword">as</span> sns</span><br><span class="line"><span class="keyword">import</span> sklearn</span><br><span class="line"><span class="keyword">from</span> sklearn <span class="keyword">import</span> linear_model</span><br><span class="line"><span class="keyword">from</span> sklearn <span class="keyword">import</span> preprocessing</span><br><span class="line"><span class="keyword">from</span> sklearn <span class="keyword">import</span> metrics</span><br><span class="line"><span class="keyword">from</span> sklearn.utils <span class="keyword">import</span> shuffle</span><br><span class="line"><span class="keyword">from</span> io <span class="keyword">import</span> StringIO</span><br><span class="line"><span class="keyword">import</span> sys</span><br><span class="line"><span class="keyword">import</span> hdf5_getters  <span class="comment"># 和数据集一起放在根目录</span></span><br><span class="line"><span class="keyword">import</span> prettytable <span class="keyword">as</span> pt</span><br><span class="line">sns.set_style(<span class="string">&#x27;whitegrid&#x27;</span>)</span><br><span class="line">%matplotlib inline</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;导入成功！&#x27;</span>)</span><br></pre></td></tr></table></figure>
<blockquote>
<p>导入成功！</p>
</blockquote>
<h1>二. 数据集的加载与展示</h1>
<h2 id="1-百万歌曲数据集">1. 百万歌曲数据集</h2>
<ul>
<li>本课程采用的数据集来自于“百万歌曲数据集（Million Song Dataset, MSD）”。
<ul>
<li>百万歌曲数据集包括了1,000,000首当代流行音乐的音频特征和元数据。
<ul>
<li>音频特征是对歌曲的音频的数据描述，包括响度、音色、音调等。</li>
<li>歌曲的<em>元数据</em>指歌曲名称、演唱者、词曲作者、发行公司、发行年份等信息，它们不能在歌曲的音频中直接体现。</li>
</ul>
</li>
</ul>
</li>
<li>每首歌用一个<code>.h5</code>文件储存其特征，所有的文件在一个目录树中分布。
<ul>
<li><code>.h5</code>文件使用<code>HDF5</code>格式储存数据，是一种层次化的数据储存格式。</li>
<li>本数据集提供了<code>hdf5_getters.py</code>模组来读取这种格式的文件（已作为附件提供，也可在数据集官网中下载）。</li>
<li>更多关于<code>HDF5</code>格式的知识可以参考维基百科：<a target="_blank" rel="noopener" href="https://en.wikipedia.org/wiki/Hierarchical_Data_Format">https://en.wikipedia.org/wiki/Hierarchical_Data_Format</a></li>
</ul>
</li>
<li>更多关于百万歌曲数据集的介绍、示例、下载等，可以参考官网：<a target="_blank" rel="noopener" href="http://millionsongdataset.com/">http://millionsongdataset.com/</a>
<ul>
<li>原始论文<em>The Million Song Dataset</em> <a target="_blank" rel="noopener" href="http://ismir2011.ismir.net/papers/OS6-1.pdf">http://ismir2011.ismir.net/papers/OS6-1.pdf</a></li>
</ul>
</li>
<li>下面，我们展示其中一首歌的特征。
<ul>
<li>我们读取一个数据文件，用数据集提供的<code>hdf5_getters</code>模组来获取其中包含的特征。</li>
<li>我们首先看一下一个数据文件中包含多少种特征，然后用表格的形式展示出来。</li>
</ul>
</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 2.1 百万歌曲数据集</span></span><br><span class="line"><span class="comment"># 2.1.1 一首歌包括多少种特征</span></span><br><span class="line">getter_list = <span class="built_in">list</span>(<span class="built_in">filter</span>(<span class="keyword">lambda</span> x: x[:<span class="number">3</span>] == <span class="string">&#x27;get&#x27;</span>, hdf5_getters.__dict__.keys()))  <span class="comment"># 特征的数量就是getter的种类</span></span><br><span class="line">number_of_features = <span class="built_in">len</span>(getter_list) </span><br><span class="line"><span class="built_in">print</span>(<span class="string">u&#x27;一首歌曲的特征数：&#x27;</span>, number_of_features)</span><br><span class="line"></span><br><span class="line"><span class="comment">#2.1.2 展示某文件的特征</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;文件TRAAABD128F429CF47.h5中的所有特征：&#x27;</span>)</span><br><span class="line">file_path = <span class="string">&#x27;TRAAABD128F429CF47.h5&#x27;</span> <span class="comment"># 数据集中的一个数据文件</span></span><br><span class="line">file = hdf5_getters.open_h5_file_read(file_path)  <span class="comment"># 读取文件</span></span><br><span class="line"></span><br><span class="line">table = pt.PrettyTable()</span><br><span class="line">table.field_names = [<span class="string">&quot;序号&quot;</span>, <span class="string">&quot;特征名&quot;</span>, <span class="string">&quot;特征类型&quot;</span>, <span class="string">&quot;值/数组形状&quot;</span>]</span><br><span class="line"><span class="keyword">for</span> cnt, key <span class="keyword">in</span> <span class="built_in">zip</span>(<span class="built_in">range</span>(<span class="number">1</span>, number_of_features+<span class="number">1</span>), getter_list):</span><br><span class="line">    func = hdf5_getters.__dict__[key]</span><br><span class="line">    <span class="keyword">if</span> <span class="built_in">isinstance</span>(func(file), np.ndarray):  <span class="comment"># 如果是数组</span></span><br><span class="line">        table.add_row([cnt, key[<span class="number">4</span>:], <span class="built_in">str</span>(<span class="built_in">type</span>(func(file)))[<span class="number">14</span>:-<span class="number">2</span>], func(file).shape]) <span class="comment"># 序号，特征名，特征类型，数组形状</span></span><br><span class="line">    <span class="keyword">else</span>:  <span class="comment"># 不是数组</span></span><br><span class="line">        table.add_row([cnt, key[<span class="number">4</span>:], <span class="built_in">str</span>(<span class="built_in">type</span>(func(file)))[<span class="number">14</span>:-<span class="number">2</span>], func(file)]) <span class="comment"># 序号，特征名，特征类型，特征值</span></span><br><span class="line"><span class="built_in">print</span>(table)</span><br><span class="line">file.close()</span><br></pre></td></tr></table></figure>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br></pre></td><td class="code"><pre><span class="line">文件TRAAABD128F429CF47.h5中的所有特征：</span><br><span class="line">+------+----------------------------+----------+-----------------------------------------+</span><br><span class="line">| 序号 |           特征名           | 特征类型 |               值/数组形状               |</span><br><span class="line">+------+----------------------------+----------+-----------------------------------------+</span><br><span class="line">|  1   |         num_songs          |  int64   |                    1                    |</span><br><span class="line">|  2   |     artist_familiarity     | float64  |            0.6306300375898077           |</span><br><span class="line">|  3   |     artist_hotttnesss      | float64  |            0.4174996449709784           |</span><br><span class="line">|  4   |         artist_id          |  bytes_  |          b&#x27;ARMJAGH1187FB546F3&#x27;          |</span><br><span class="line">|  5   |        artist_mbid         |  bytes_  | b&#x27;1c78ab62-db33-4433-8d0b-7c8dcf1849c2&#x27; |</span><br><span class="line">|  6   |      artist_playmeid       |  int32   |                  22066                  |</span><br><span class="line">|  7   |     artist_7digitalid      |  int32   |                   1998                  |</span><br><span class="line">|  8   |      artist_latitude       | float64  |                 35.14968                |</span><br><span class="line">|  9   |      artist_longitude      | float64  |                -90.04892                |</span><br><span class="line">|  10  |      artist_location       |  bytes_  |              b&#x27;Memphis, TN&#x27;             |</span><br><span class="line">|  11  |        artist_name         |  bytes_  |             b&#x27;The Box Tops&#x27;             |</span><br><span class="line">|  12  |          release           |  bytes_  |              b&#x27;Dimensions&#x27;              |</span><br><span class="line">|  13  |     release_7digitalid     |  int32   |                  300822                 |</span><br><span class="line">|  14  |          song_id           |  bytes_  |          b&#x27;SOCIWDW12A8C13D406&#x27;          |</span><br><span class="line">|  15  |      song_hotttnesss       | float64  |                   nan                   |</span><br><span class="line">|  16  |           title            |  bytes_  |               b&#x27;Soul Deep&#x27;              |</span><br><span class="line">|  17  |      track_7digitalid      |  int32   |                 3400270                 |</span><br><span class="line">|  18  |      similar_artists       | ndarray  |                  (100,)                 |</span><br><span class="line">|  19  |        artist_terms        | ndarray  |                  (38,)                  |</span><br><span class="line">|  20  |     artist_terms_freq      | ndarray  |                  (38,)                  |</span><br><span class="line">|  21  |    artist_terms_weight     | ndarray  |                  (38,)                  |</span><br><span class="line">|  22  |    analysis_sample_rate    |  int32   |                  22050                  |</span><br><span class="line">|  23  |         audio_md5          |  bytes_  |   b&#x27;bb9771eeef3d5b204a3c55e690f52a91&#x27;   |</span><br><span class="line">|  24  |        danceability        | float64  |                   0.0                   |</span><br><span class="line">|  25  |          duration          | float64  |                148.03546                |</span><br><span class="line">|  26  |       end_of_fade_in       | float64  |                  0.148                  |</span><br><span class="line">|  27  |           energy           | float64  |                   0.0                   |</span><br><span class="line">|  28  |            key             |  int32   |                    6                    |</span><br><span class="line">|  29  |       key_confidence       | float64  |                  0.169                  |</span><br><span class="line">|  30  |          loudness          | float64  |                  -9.843                 |</span><br><span class="line">|  31  |            mode            |  int32   |                    0                    |</span><br><span class="line">|  32  |      mode_confidence       | float64  |                   0.43                  |</span><br><span class="line">|  33  |     start_of_fade_out      | float64  |                 137.915                 |</span><br><span class="line">|  34  |           tempo            | float64  |                 121.274                 |</span><br><span class="line">|  35  |       time_signature       |  int32   |                    4                    |</span><br><span class="line">|  36  | time_signature_confidence  | float64  |                  0.384                  |</span><br><span class="line">|  37  |          track_id          |  bytes_  |          b&#x27;TRAAABD128F429CF47&#x27;          |</span><br><span class="line">|  38  |       segments_start       | ndarray  |                  (550,)                 |</span><br><span class="line">|  39  |    segments_confidence     | ndarray  |                  (550,)                 |</span><br><span class="line">|  40  |      segments_pitches      | ndarray  |                (550, 12)                |</span><br><span class="line">|  41  |      segments_timbre       | ndarray  |                (550, 12)                |</span><br><span class="line">|  42  |   segments_loudness_max    | ndarray  |                  (550,)                 |</span><br><span class="line">|  43  | segments_loudness_max_time | ndarray  |                  (550,)                 |</span><br><span class="line">|  44  |  segments_loudness_start   | ndarray  |                  (550,)                 |</span><br><span class="line">|  45  |       sections_start       | ndarray  |                   (9,)                  |</span><br><span class="line">|  46  |    sections_confidence     | ndarray  |                   (9,)                  |</span><br><span class="line">|  47  |        beats_start         | ndarray  |                  (296,)                 |</span><br><span class="line">|  48  |      beats_confidence      | ndarray  |                  (296,)                 |</span><br><span class="line">|  49  |         bars_start         | ndarray  |                  (73,)                  |</span><br><span class="line">|  50  |      bars_confidence       | ndarray  |                  (73,)                  |</span><br><span class="line">|  51  |        tatums_start        | ndarray  |                  (591,)                 |</span><br><span class="line">|  52  |     tatums_confidence      | ndarray  |                  (591,)                 |</span><br><span class="line">|  53  |       artist_mbtags        | ndarray  |                   (1,)                  |</span><br><span class="line">|  54  |    artist_mbtags_count     | ndarray  |                   (1,)                  |</span><br><span class="line">|  55  |            year            |  int32   |                   1969                  |</span><br><span class="line">+------+----------------------------+----------+-----------------------------------------+</span><br></pre></td></tr></table></figure>
<ul>
<li>从上面的结果可以看出：
<ul>
<li>每个样本都有55种特征。</li>
<li>从数据类型看，有的特征值是一个实数或字符串，有的特征值是一个array。</li>
<li>有的样本的部分特征值缺失了。（比如这里的song_hotttness，即歌曲热度，是nan，意为无效数据。）</li>
</ul>
</li>
</ul>
<h2 id="2-数据集的加载">2. 数据集的加载</h2>
<ul>
<li>在本实验中，我们要使用到百万歌曲数据集中的音频特征和发行年份。我们使用的是<em>YearPredictionMSD Data Set</em>。
<ul>
<li>它是百万歌曲数据集的一个子集，包含了其中的音频特征和年份，由加州大学尔湾分校（UCI）的研究人员创建。</li>
<li>这个数据集没有重复的样本，也没有缺失的数据。</li>
<li>每一个样本都包含90个音频特征。这些特征包括12个平均值和78个协方差。为了时间和空间的效率，我们仅保留前12个音频特征，即12个平均值。</li>
<li>来源：<a target="_blank" rel="noopener" href="https://archive.ics.uci.edu/ml/datasets/yearpredictionmsd">https://archive.ics.uci.edu/ml/datasets/yearpredictionmsd</a></li>
</ul>
</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 数据集的加载</span></span><br><span class="line"><span class="comment">################################## Question 1 ########################################</span></span><br><span class="line"><span class="comment"># 要求：</span></span><br><span class="line"><span class="comment">#   - 加载数据集year_prediction.csv，保存为变量data，类型：DataFrame </span></span><br><span class="line"><span class="comment">#   - 将数据集的特征部分另存为变量X，类型：DataFrame</span></span><br><span class="line"><span class="comment">#   - 将数据集的标签部分另存为变量Y，类型：Series</span></span><br><span class="line"><span class="comment">############################## Start of Your code ####################################</span></span><br><span class="line">data = pd.read_csv(<span class="string">&#x27;year_prediction.csv&#x27;</span>)</span><br><span class="line">X = data.iloc[:,<span class="number">1</span>:]</span><br><span class="line">Y = data.label</span><br><span class="line"></span><br><span class="line"><span class="comment">############################### End of Your Code #####################################</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">assert</span> X.shape[<span class="number">0</span>] == Y.size  <span class="comment"># 检验X和Y的样本数是否一致</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;读取成功！&#x27;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 查看数据集的行数和列数</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;数据集中的样本数：&#x27;</span>, X.shape[<span class="number">0</span>])</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;每个样本的特征数：&#x27;</span>, X.shape[<span class="number">1</span>])</span><br><span class="line"></span><br><span class="line"><span class="comment"># 查看数据集的前5个样本</span></span><br><span class="line">data.head(<span class="number">5</span>)</span><br></pre></td></tr></table></figure>
<blockquote>
<p>读取成功！<br>
数据集中的样本数： 515345<br>
每个样本的特征数： 12</p>
</blockquote>
<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }
<p>.dataframe tbody tr th {<br>
vertical-align: top;<br>
}</p>
<p>.dataframe thead th {<br>
text-align: right;<br>
}<br>
</style></p>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>label</th>
      <th>TimbreAvg1</th>
      <th>TimbreAvg2</th>
      <th>TimbreAvg3</th>
      <th>TimbreAvg4</th>
      <th>TimbreAvg5</th>
      <th>TimbreAvg6</th>
      <th>TimbreAvg7</th>
      <th>TimbreAvg8</th>
      <th>TimbreAvg9</th>
      <th>TimbreAvg10</th>
      <th>TimbreAvg11</th>
      <th>TimbreAvg12</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>2001</td>
      <td>49.94357</td>
      <td>21.47114</td>
      <td>73.07750</td>
      <td>8.74861</td>
      <td>-17.40628</td>
      <td>-13.09905</td>
      <td>-25.01202</td>
      <td>-12.23257</td>
      <td>7.83089</td>
      <td>-2.46783</td>
      <td>3.32136</td>
      <td>-2.31521</td>
    </tr>
    <tr>
      <th>1</th>
      <td>2001</td>
      <td>48.73215</td>
      <td>18.42930</td>
      <td>70.32679</td>
      <td>12.94636</td>
      <td>-10.32437</td>
      <td>-24.83777</td>
      <td>8.76630</td>
      <td>-0.92019</td>
      <td>18.76548</td>
      <td>4.59210</td>
      <td>2.21920</td>
      <td>0.34006</td>
    </tr>
    <tr>
      <th>2</th>
      <td>2001</td>
      <td>50.95714</td>
      <td>31.85602</td>
      <td>55.81851</td>
      <td>13.41693</td>
      <td>-6.57898</td>
      <td>-18.54940</td>
      <td>-3.27872</td>
      <td>-2.35035</td>
      <td>16.07017</td>
      <td>1.39518</td>
      <td>2.73553</td>
      <td>0.82804</td>
    </tr>
    <tr>
      <th>3</th>
      <td>2001</td>
      <td>48.24750</td>
      <td>-1.89837</td>
      <td>36.29772</td>
      <td>2.58776</td>
      <td>0.97170</td>
      <td>-26.21683</td>
      <td>5.05097</td>
      <td>-10.34124</td>
      <td>3.55005</td>
      <td>-6.36304</td>
      <td>6.63016</td>
      <td>-3.35142</td>
    </tr>
    <tr>
      <th>4</th>
      <td>2001</td>
      <td>50.97020</td>
      <td>42.20998</td>
      <td>67.09964</td>
      <td>8.46791</td>
      <td>-15.85279</td>
      <td>-16.81409</td>
      <td>-12.48207</td>
      <td>-9.37636</td>
      <td>12.63699</td>
      <td>0.93609</td>
      <td>1.60923</td>
      <td>2.19223</td>
    </tr>
  </tbody>
</table>
</div>
<ul>
<li>可见，数据集中有515345个样本，每个样本除了预测目标year以外，有12个特征。</li>
<li>这些特征包括12个平均值（TimbreAvg1~12）。
<ul>
<li>每一个样本包含每首歌都被分为12个片段（segment），每个片段用一个12维的向量来表示它的音色（timbre）。对每个12维的向量求平均值，那么一首歌可以得到12个平均值。</li>
</ul>
</li>
</ul>
<h2 id="3-数据集的展示">3. 数据集的展示</h2>
<ul>
<li>我们首先来以10年为一个单位，展示数据集中歌曲年代的分布。</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 每个十年的歌曲数</span></span><br><span class="line"><span class="comment">################################## Question 2 ########################################</span></span><br><span class="line"><span class="comment"># 要求：</span></span><br><span class="line"><span class="comment">#   - 首先根据歌曲年份计算年代，并将歌曲年代作为一个额外的特征decade加入变量data中。</span></span><br><span class="line"><span class="comment">#     - 例如一首歌的发行年份是1986，那么它的decade列的数据应为1980</span></span><br><span class="line"><span class="comment">#   - 绘制一幅统计图，展示每个十年的歌曲数。</span></span><br><span class="line"><span class="comment"># 提示：</span></span><br><span class="line"><span class="comment">#   - 计数和绘图可使用seaborn.countplot()</span></span><br><span class="line"><span class="comment">############################## Start of Your code ####################################</span></span><br><span class="line">data[<span class="string">&#x27;decade&#x27;</span>] = [i//<span class="number">10</span>*<span class="number">10</span> <span class="keyword">for</span> i <span class="keyword">in</span> Y]</span><br><span class="line">sns.countplot(y=data[<span class="string">&#x27;decade&#x27;</span>])</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">############################### End of Your Code #####################################</span></span><br><span class="line"></span><br><span class="line">plt.title(<span class="string">&#x27;Distribution of Release Year&#x27;</span>,fontsize=<span class="number">20</span>)</span><br><span class="line">plt.xlabel(<span class="string">&#x27;Count of Songs&#x27;</span>,fontsize=<span class="number">14</span>)</span><br><span class="line">plt.ylabel(<span class="string">&#x27;Decade&#x27;</span>,fontsize=<span class="number">14</span>)</span><br></pre></td></tr></table></figure>
<p>Text(0, 0.5, ‘Decade’)<br>
<img src="https://img.zsaqwq.com/images/2022/03/28/ML_exp1_11_1.png" alt="png"></p>
<ul>
<li>我们的任务，就是使用这些样本进行训练。</li>
<li>我们想要建立起一个回归模型，即对于一个未知的样本：
<ul>
<li>输入：由12个特征值组成的向量</li>
<li>输出：音乐的发行年代year</li>
</ul>
</li>
</ul>
<h1>三. 特征筛选与预处理</h1>
<h2 id="1-特征筛选">1. 特征筛选</h2>
<ul>
<li>根据第二部分的结果，我们可以发现，在百万歌曲数据集中：
<ol>
<li>不是所有特征都是数字，能方便地进行数学计算；</li>
<li>不是所有特征都与我们的目标（预测年代）密切相关；</li>
</ol>
</li>
<li>因此，需要对这些特征进行<em><strong>特征筛选</strong></em>，即：保留部分特征作为后续分析的输入。</li>
<li>我们采用的数据集YearPredictionMSD Data Set，已对百万歌曲数据集进行了数据筛选。
<ul>
<li>只保留了和音色timbre有关的特征，然后进行计算，得到平均值和协方差。</li>
</ul>
</li>
</ul>
<h2 id="2-特征的数值分布展示">2. 特征的数值分布展示</h2>
<ul>
<li>我们观察一下样本中各维度的数值分布情况。
<ul>
<li>首先看一下各维度的统计数据：平均值、标准差、最小值、25%、50%、75%、最大值</li>
</ul>
</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">X.describe()</span><br></pre></td></tr></table></figure>
<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }
<p>.dataframe tbody tr th {<br>
vertical-align: top;<br>
}</p>
<p>.dataframe thead th {<br>
text-align: right;<br>
}<br>
</style></p>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>TimbreAvg1</th>
      <th>TimbreAvg2</th>
      <th>TimbreAvg3</th>
      <th>TimbreAvg4</th>
      <th>TimbreAvg5</th>
      <th>TimbreAvg6</th>
      <th>TimbreAvg7</th>
      <th>TimbreAvg8</th>
      <th>TimbreAvg9</th>
      <th>TimbreAvg10</th>
      <th>TimbreAvg11</th>
      <th>TimbreAvg12</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>count</th>
      <td>515345.000000</td>
      <td>515345.000000</td>
      <td>515345.000000</td>
      <td>515345.000000</td>
      <td>515345.000000</td>
      <td>515345.000000</td>
      <td>515345.000000</td>
      <td>515345.000000</td>
      <td>515345.000000</td>
      <td>515345.000000</td>
      <td>515345.000000</td>
      <td>515345.000000</td>
    </tr>
    <tr>
      <th>mean</th>
      <td>43.387126</td>
      <td>1.289554</td>
      <td>8.658347</td>
      <td>1.164124</td>
      <td>-6.553601</td>
      <td>-9.521975</td>
      <td>-2.391089</td>
      <td>-1.793236</td>
      <td>3.727876</td>
      <td>1.882385</td>
      <td>-0.146527</td>
      <td>2.546063</td>
    </tr>
    <tr>
      <th>std</th>
      <td>6.067558</td>
      <td>51.580351</td>
      <td>35.268585</td>
      <td>16.322790</td>
      <td>22.860785</td>
      <td>12.857751</td>
      <td>14.571873</td>
      <td>7.963827</td>
      <td>10.582861</td>
      <td>6.530232</td>
      <td>4.370848</td>
      <td>8.320190</td>
    </tr>
    <tr>
      <th>min</th>
      <td>1.749000</td>
      <td>-337.092500</td>
      <td>-301.005060</td>
      <td>-154.183580</td>
      <td>-181.953370</td>
      <td>-81.794290</td>
      <td>-188.214000</td>
      <td>-72.503850</td>
      <td>-126.479040</td>
      <td>-41.631660</td>
      <td>-69.680870</td>
      <td>-94.041960</td>
    </tr>
    <tr>
      <th>25%</th>
      <td>39.954690</td>
      <td>-26.059520</td>
      <td>-11.462710</td>
      <td>-8.487500</td>
      <td>-20.666450</td>
      <td>-18.440990</td>
      <td>-10.780600</td>
      <td>-6.468420</td>
      <td>-2.293660</td>
      <td>-2.444850</td>
      <td>-2.652090</td>
      <td>-2.550060</td>
    </tr>
    <tr>
      <th>50%</th>
      <td>44.258500</td>
      <td>8.417850</td>
      <td>10.476320</td>
      <td>-0.652840</td>
      <td>-6.007770</td>
      <td>-11.188390</td>
      <td>-2.046670</td>
      <td>-1.736450</td>
      <td>3.822310</td>
      <td>1.783520</td>
      <td>-0.097950</td>
      <td>2.313700</td>
    </tr>
    <tr>
      <th>75%</th>
      <td>47.833890</td>
      <td>36.124010</td>
      <td>29.764820</td>
      <td>8.787540</td>
      <td>7.741870</td>
      <td>-2.388960</td>
      <td>6.508580</td>
      <td>2.913450</td>
      <td>9.961820</td>
      <td>6.147220</td>
      <td>2.435660</td>
      <td>7.360330</td>
    </tr>
    <tr>
      <th>max</th>
      <td>61.970140</td>
      <td>384.065730</td>
      <td>322.851430</td>
      <td>335.771820</td>
      <td>262.068870</td>
      <td>166.236890</td>
      <td>172.402680</td>
      <td>126.741270</td>
      <td>146.297950</td>
      <td>60.345350</td>
      <td>88.020820</td>
      <td>87.913240</td>
    </tr>
  </tbody>
</table>
</div>
<ul>
<li>为了直观地展现特征的数值分布，下面绘制每个维度的分布直方图
<ul>
<li>每一幅图的横坐标代表值的大小，纵坐标代表频数/分度值。</li>
</ul>
</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">fig = plt.figure(figsize=(<span class="number">12</span>,<span class="number">10</span>))  <span class="comment"># 新建一个12*10的画布</span></span><br><span class="line">fig.suptitle(<span class="string">&quot;Distribution of features&quot;</span>, fontsize=<span class="number">20</span>)</span><br><span class="line">plt.subplots_adjust(hspace=<span class="number">0.4</span>)</span><br><span class="line"><span class="comment"># 画12个子图</span></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">12</span>):</span><br><span class="line">    ax = plt.subplot(<span class="number">4</span>,<span class="number">3</span>,i+<span class="number">1</span>)</span><br><span class="line">    sns.distplot(X.iloc[:,i])  <span class="comment"># 调用seaborn中的直方图</span></span><br><span class="line">    plt.xlabel(<span class="string">f&#x27;Feature <span class="subst">&#123;i+<span class="number">1</span>&#125;</span>&#x27;</span>, fontsize=<span class="number">14</span>)</span><br></pre></td></tr></table></figure>
<p><img src="https://img.zsaqwq.com/images/2022/03/28/ML_exp1_17_1.png" alt="png"></p>
<ul>
<li>可见：各个维度的数值范围差异是很大的，为此我们需要进行<em><strong>特征缩放</strong></em>。</li>
</ul>
<h2 id="3-特征缩放">3. 特征缩放</h2>
<ul>
<li>特征缩放：将各项特征的值缩放到相同或相近的大小范围。</li>
<li>为什么要进行特征缩放？
<ul>
<li>模型在训练时会主要受数值大的特征影响</li>
<li>如果不进行特征缩放，回归分析的训练时间会变长</li>
<li>我们接下来使用的模型对数据的规模特别敏感，不缩放就很难训练</li>
</ul>
</li>
<li>特征缩放的方法：
<ol>
<li><em><strong>标准化（Standardization）</strong></em>：一般指使数据的均值为0，方差为1
<ul>
<li>有时也指将数据的数值范围压缩到一个固定的范围，比如<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="false">[</mo><mn>0</mn><mo separator="true">,</mo><mn>1</mn><mo stretchy="false">]</mo></mrow><annotation encoding="application/x-tex">[0, 1]</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mopen">[</span><span class="mord">0</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord">1</span><span class="mclose">]</span></span></span></span></li>
</ul>
</li>
<li><em><strong>归一化（Normalization）</strong></em>：使数据的特征向量缩放到单位长度</li>
</ol>
</li>
<li>在本次实验中，我们选用<code>scikit-learn</code>中内建的特征缩放方法来处理数据
<ul>
<li>可以参考 <a target="_blank" rel="noopener" href="https://scikit-learn.org/stable/modules/preprocessing.html">https://scikit-learn.org/stable/modules/preprocessing.html</a> 中6.3.1-3中的介绍</li>
</ul>
</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 3.3 特征缩放</span></span><br><span class="line"><span class="comment"># 使用sklearn.preprocessing模组中的方法，对数据进行标准化</span></span><br><span class="line"><span class="comment">################################### Question 2 #########################################</span></span><br><span class="line"><span class="comment"># 要求：</span></span><br><span class="line"><span class="comment">#   - 对X中的数据进行标准化，将缩放后的数据存为X_scaled，类型：DataFrame</span></span><br><span class="line"><span class="comment">#     - 使用sklearn.preprocessing中的特征缩放方法</span></span><br><span class="line"><span class="comment"># 提示：</span></span><br><span class="line"><span class="comment">#   - 参考上文提到的网址，选一个合适的Scaler</span></span><br><span class="line"><span class="comment">############################### Start of Your Code #####################################</span></span><br><span class="line">scaler = preprocessing.MinMaxScaler()</span><br><span class="line">X_scaled = pd.DataFrame(scaler.fit_transform(X))</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">################################ End of Your Code ######################################</span></span><br><span class="line"><span class="comment"># 展示各特征的统计量</span></span><br><span class="line">X_scaled.describe()</span><br></pre></td></tr></table></figure>
<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }
<p>.dataframe tbody tr th {<br>
vertical-align: top;<br>
}</p>
<p>.dataframe thead th {<br>
text-align: right;<br>
}<br>
</style></p>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>0</th>
      <th>1</th>
      <th>2</th>
      <th>3</th>
      <th>4</th>
      <th>5</th>
      <th>6</th>
      <th>7</th>
      <th>8</th>
      <th>9</th>
      <th>10</th>
      <th>11</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>count</th>
      <td>515345.000000</td>
      <td>515345.000000</td>
      <td>515345.000000</td>
      <td>515345.000000</td>
      <td>515345.000000</td>
      <td>515345.000000</td>
      <td>515345.000000</td>
      <td>515345.000000</td>
      <td>515345.000000</td>
      <td>515345.000000</td>
      <td>515345.000000</td>
      <td>515345.000000</td>
    </tr>
    <tr>
      <th>mean</th>
      <td>0.691420</td>
      <td>0.469220</td>
      <td>0.496370</td>
      <td>0.317065</td>
      <td>0.395025</td>
      <td>0.291384</td>
      <td>0.515292</td>
      <td>0.354893</td>
      <td>0.477338</td>
      <td>0.426704</td>
      <td>0.440923</td>
      <td>0.530834</td>
    </tr>
    <tr>
      <th>std</th>
      <td>0.100755</td>
      <td>0.071524</td>
      <td>0.056533</td>
      <td>0.033315</td>
      <td>0.051486</td>
      <td>0.051839</td>
      <td>0.040408</td>
      <td>0.039970</td>
      <td>0.038797</td>
      <td>0.064036</td>
      <td>0.027716</td>
      <td>0.045727</td>
    </tr>
    <tr>
      <th>min</th>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
    </tr>
    <tr>
      <th>25%</th>
      <td>0.634423</td>
      <td>0.431296</td>
      <td>0.464117</td>
      <td>0.297366</td>
      <td>0.363241</td>
      <td>0.255425</td>
      <td>0.492028</td>
      <td>0.331428</td>
      <td>0.455263</td>
      <td>0.384271</td>
      <td>0.425035</td>
      <td>0.502827</td>
    </tr>
    <tr>
      <th>50%</th>
      <td>0.705890</td>
      <td>0.479105</td>
      <td>0.499284</td>
      <td>0.313357</td>
      <td>0.396254</td>
      <td>0.284665</td>
      <td>0.516247</td>
      <td>0.355178</td>
      <td>0.477685</td>
      <td>0.425735</td>
      <td>0.441231</td>
      <td>0.529557</td>
    </tr>
    <tr>
      <th>75%</th>
      <td>0.765261</td>
      <td>0.517524</td>
      <td>0.530202</td>
      <td>0.332624</td>
      <td>0.427220</td>
      <td>0.320143</td>
      <td>0.539971</td>
      <td>0.378515</td>
      <td>0.500192</td>
      <td>0.468526</td>
      <td>0.457297</td>
      <td>0.557293</td>
    </tr>
    <tr>
      <th>max</th>
      <td>1.000000</td>
      <td>1.000000</td>
      <td>1.000000</td>
      <td>1.000000</td>
      <td>1.000000</td>
      <td>1.000000</td>
      <td>1.000000</td>
      <td>1.000000</td>
      <td>1.000000</td>
      <td>1.000000</td>
      <td>1.000000</td>
      <td>1.000000</td>
    </tr>
  </tbody>
</table>
</div>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">fig = plt.figure(figsize=(<span class="number">12</span>,<span class="number">10</span>))  <span class="comment"># 新建一个12*10的画布</span></span><br><span class="line">fig.suptitle(<span class="string">&quot;Distribution of features&quot;</span>, fontsize=<span class="number">20</span>)</span><br><span class="line">plt.subplots_adjust(hspace=<span class="number">0.4</span>)</span><br><span class="line"><span class="comment"># 画12个子图</span></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">12</span>):</span><br><span class="line">    ax = plt.subplot(<span class="number">4</span>,<span class="number">3</span>,i+<span class="number">1</span>)</span><br><span class="line">    sns.distplot(X.iloc[:,i])  <span class="comment"># 调用seaborn中的直方图</span></span><br><span class="line">    plt.xlabel(<span class="string">f&#x27;Feature <span class="subst">&#123;i+<span class="number">1</span>&#125;</span>&#x27;</span>, fontsize=<span class="number">14</span>)</span><br></pre></td></tr></table></figure>
<p><img src="https://img.zsaqwq.com/images/2022/03/28/ML_exp1_20_1.png" alt="png"></p>
<h2 id="4-特征展示">4. 特征展示</h2>
<ul>
<li>下面我们尝试直观展示一下不同年代音乐之间的特征差异
<ul>
<li>随机抽取四个特征，做散点图</li>
</ul>
</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 3.4 特征展示</span></span><br><span class="line">X_sample=X.sample(n=<span class="number">4</span>, axis=<span class="number">1</span>)</span><br><span class="line">X_sample[<span class="string">&#x27;decade&#x27;</span>]=data[<span class="string">&#x27;decade&#x27;</span>]</span><br><span class="line">X_sample=X_sample.sample(<span class="number">1000</span>)</span><br><span class="line">sns.pairplot(data=X_sample, hue=<span class="string">&#x27;decade&#x27;</span>, <span class="built_in">vars</span>=X_sample.columns[:-<span class="number">1</span>])</span><br></pre></td></tr></table></figure>
<p>&lt;seaborn.axisgrid.PairGrid at 0x1a396749f10&gt;<br>
<img src="https://img.zsaqwq.com/images/2022/03/28/ML_exp1_22_1.png" alt="png"></p>
<ul>
<li>用热力图（heatmap）展现不同年代音乐的特征向量。
<ul>
<li>我们剔除样本过少的20~40年代音乐，然后从每个年代的音乐中取相同数量，对它们的特征向量求平均值。</li>
<li>颜色越浅代表这一年代音乐的某一平均特征的值越大。</li>
</ul>
</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">data_t = data[data.decade&gt;<span class="number">1940</span>]  <span class="comment"># 剔除40年代及以前</span></span><br><span class="line">min_samples = data_t.decade.value_counts().<span class="built_in">min</span>()</span><br><span class="line">decades = data_t.decade.unique()</span><br><span class="line">data_sampled = pd.DataFrame(columns=data_t.columns)</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> decade <span class="keyword">in</span> decades:</span><br><span class="line">    data_sampled = data_sampled.append(data_t[data_t.decade==decade].sample(min_samples))</span><br><span class="line">data_sampled.decade = data_sampled.decade.astype(<span class="built_in">int</span>)</span><br><span class="line"></span><br><span class="line">labels = [<span class="string">&quot;&#123;:02d&#125;&#x27;s&#x27;&quot;</span>.<span class="built_in">format</span>(l%<span class="number">100</span>) <span class="keyword">for</span> l <span class="keyword">in</span> <span class="built_in">sorted</span>(data_sampled.decade.unique())]</span><br><span class="line">fig, ax = plt.subplots(figsize=(<span class="number">12</span>,<span class="number">5</span>)) </span><br><span class="line">sns.heatmap(data_sampled.groupby([<span class="string">&#x27;decade&#x27;</span>]).mean(), yticklabels=labels)</span><br><span class="line">plt.ylabel(<span class="string">&quot;Decade&quot;</span>)</span><br><span class="line">plt.xlabel(<span class="string">&quot;Average of features&quot;</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
<p><img src="https://img.zsaqwq.com/images/2022/03/28/ML_exp1_24_0.png" alt="png"></p>
<h2 id="5-数据集划分">5. 数据集划分</h2>
<ul>
<li>回归问题是一类<em><strong>监督学习</strong></em>的问题
<ul>
<li>监督学习需要有训练数据集</li>
</ul>
</li>
<li>我们把数据集划分为<em><strong>训练数据集</strong></em>和<em><strong>测试数据集</strong></em>
<ul>
<li>模型仅使用训练集中的数据进行训练</li>
<li>训练完成后，然后在测试集中进行测试</li>
</ul>
</li>
<li>一般来说，需要指定训练集和测试集的比例。
<ul>
<li>本课程采用的数据集推荐的划分是：前463,715个样本为训练集，后51,630个样本为测试集</li>
</ul>
</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 3.5 数据集划分</span></span><br><span class="line"><span class="comment">################################### Question 3 #########################################</span></span><br><span class="line"><span class="comment"># 要求：</span></span><br><span class="line"><span class="comment">#   - 对数据进行数据集划分，要满足前463,715个样本为训练集，后51,630个样本为测试集</span></span><br><span class="line"><span class="comment">#   - 将训练集特征、测试集特征分别存为x_train, x_test，类型：DataFrame</span></span><br><span class="line"><span class="comment">#   - 将训练集目标、测试集目标分别存为y_train, y_test，类型：Series</span></span><br><span class="line"><span class="comment">############################### Start of Your Code #####################################</span></span><br><span class="line">x_train=X_scaled[:<span class="number">463715</span>]</span><br><span class="line">x_test=X_scaled[<span class="number">463715</span>:]</span><br><span class="line">y_train=Y[:<span class="number">463715</span>]</span><br><span class="line">y_test=Y[<span class="number">463715</span>:]</span><br><span class="line"></span><br><span class="line"><span class="comment">################################ End of Your Code ######################################</span></span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(x_train.shape)</span><br><span class="line"><span class="built_in">print</span>(y_train.shape)</span><br><span class="line"><span class="built_in">print</span>(x_test.shape)</span><br><span class="line"><span class="built_in">print</span>(y_test.shape)</span><br></pre></td></tr></table></figure>
<blockquote>
<p>(463715, 12)<br>
(463715,)<br>
(51630, 12)<br>
(51630,)</p>
</blockquote>
<h1>四. 线性回归</h1>
<h2 id="1-线性回归">1. 线性回归</h2>
<ul>
<li><strong>回归方程（regression equation）</strong>，又叫<strong>假设（hypothesis）<strong>或</strong>预测函数</strong>。
<ul>
<li>回归方程就是通过学习建立起来的，输入的特征向量与输出的关系式。</li>
<li>有了回归方程，将一个未知样本的特征<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mover accent="true"><mi>x</mi><mo>⃗</mo></mover></mrow><annotation encoding="application/x-tex">\vec x</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.714em;"></span><span class="mord accent"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.714em;"><span style="top:-3em;"><span class="pstrut" style="height:3em;"></span><span class="mord mathnormal">x</span></span><span style="top:-3em;"><span class="pstrut" style="height:3em;"></span><span class="accent-body" style="left:-0.2077em;"><span class="overlay" style="height:0.714em;width:0.471em;"><svg xmlns="http://www.w3.org/2000/svg" width='0.471em' height='0.714em' style='width:0.471em' viewBox='0 0 471 714' preserveAspectRatio='xMinYMin'><path d='M377 20c0-5.333 1.833-10 5.5-14S391 0 397 0c4.667 0 8.667 1.667 12 5
3.333 2.667 6.667 9 10 19 6.667 24.667 20.333 43.667 41 57 7.333 4.667 11
10.667 11 18 0 6-1 10-3 12s-6.667 5-14 9c-28.667 14.667-53.667 35.667-75 63
-1.333 1.333-3.167 3.5-5.5 6.5s-4 4.833-5 5.5c-1 .667-2.5 1.333-4.5 2s-4.333 1
-7 1c-4.667 0-9.167-1.833-13.5-5.5S337 184 337 178c0-12.667 15.667-32.333 47-59
H213l-171-1c-8.667-6-13-12.333-13-19 0-4.667 4.333-11.333 13-20h359
c-16-25.333-24-45-24-59z'/></svg></span></span></span></span></span></span></span></span></span></span>输入进去，就可得到他的输出值<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>y</mi></mrow><annotation encoding="application/x-tex">y</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.625em;vertical-align:-0.1944em;"></span><span class="mord mathnormal" style="margin-right:0.03588em;">y</span></span></span></span>的预测值。</li>
<li>当回归方程是一个线性函数时，称为<em><strong>线性回归</strong></em>。</li>
</ul>
</li>
</ul>
<p><span class="katex-display"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><msub><mi>h</mi><mi>θ</mi></msub><mo stretchy="false">(</mo><mi>x</mi><mo stretchy="false">)</mo><mo>=</mo><msub><mi>θ</mi><mn>0</mn></msub><mo>+</mo><msub><mi>θ</mi><mn>1</mn></msub><msub><mi>x</mi><mn>1</mn></msub><mo>+</mo><msub><mi>θ</mi><mn>2</mn></msub><msub><mi>x</mi><mn>2</mn></msub><mo>+</mo><mo>⋯</mo><mo>+</mo><msub><mi>θ</mi><mi>n</mi></msub><msub><mi>x</mi><mi>n</mi></msub><mo>=</mo><msup><mi>θ</mi><mi>T</mi></msup><mi>x</mi></mrow><annotation encoding="application/x-tex">h_\theta(x)=\theta_0+\theta_1x_1+\theta_2x_2+\dots+\theta_nx_n=\theta^Tx
</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord"><span class="mord mathnormal">h</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3361em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.02778em;">θ</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mopen">(</span><span class="mord mathnormal">x</span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:0.8444em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.02778em;">θ</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3011em;"><span style="top:-2.55em;margin-left:-0.0278em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">0</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222em;"></span></span><span class="base"><span class="strut" style="height:0.8444em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.02778em;">θ</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3011em;"><span style="top:-2.55em;margin-left:-0.0278em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">1</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mord"><span class="mord mathnormal">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3011em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">1</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222em;"></span></span><span class="base"><span class="strut" style="height:0.8444em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.02778em;">θ</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3011em;"><span style="top:-2.55em;margin-left:-0.0278em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mord"><span class="mord mathnormal">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3011em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222em;"></span></span><span class="base"><span class="strut" style="height:0.6667em;vertical-align:-0.0833em;"></span><span class="minner">⋯</span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222em;"></span></span><span class="base"><span class="strut" style="height:0.8444em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.02778em;">θ</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.1514em;"><span style="top:-2.55em;margin-left:-0.0278em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">n</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mord"><span class="mord mathnormal">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.1514em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">n</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:0.8913em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.02778em;">θ</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8913em;"><span style="top:-3.113em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.13889em;">T</span></span></span></span></span></span></span></span><span class="mord mathnormal">x</span></span></span></span></span></p>
<h2 id="2-代价函数">2. 代价函数</h2>
<ul>
<li>
<p>如何评估模型的好坏</p>
<ul>
<li><em><strong>代价函数</strong></em>：评估真实值与预测值之间的差异</li>
<li>在线性回归中，我们常用平方损失（squared loss）来描述误差</li>
</ul>
<p><span class="katex-display"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><mi>J</mi><mo stretchy="false">(</mo><mi>θ</mi><mo stretchy="false">)</mo><mo>=</mo><mfrac><mn>1</mn><mrow><mn>2</mn><mi>m</mi></mrow></mfrac><munderover><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>m</mi></munderover><mo stretchy="false">(</mo><msub><mi>h</mi><mi>θ</mi></msub><mo stretchy="false">(</mo><msup><mi>x</mi><mrow><mo stretchy="false">(</mo><mi>i</mi><mo stretchy="false">)</mo></mrow></msup><mo stretchy="false">)</mo><mo>−</mo><msup><mi>y</mi><mrow><mo stretchy="false">(</mo><mi>i</mi><mo stretchy="false">)</mo></mrow></msup><msup><mo stretchy="false">)</mo><mn>2</mn></msup><mo separator="true">,</mo><mspace width="1em"/><mrow><mtext>(</mtext><mstyle scriptlevel="0" displaystyle="false"><mi>m</mi></mstyle><mtext> 为样本数)</mtext></mrow></mrow><annotation encoding="application/x-tex">J(\theta)=\frac{1}{2m}\sum\limits_{i=1}^{m}(h_\theta(x^{(i)})-y^{(i)})^2,\quad \text{($m$ 为样本数)}
</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathnormal" style="margin-right:0.09618em;">J</span><span class="mopen">(</span><span class="mord mathnormal" style="margin-right:0.02778em;">θ</span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:2.9291em;vertical-align:-1.2777em;"></span><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.3214em;"><span style="top:-2.314em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord">2</span><span class="mord mathnormal">m</span></span></span><span style="top:-3.23em;"><span class="pstrut" style="height:3em;"></span><span class="frac-line" style="border-bottom-width:0.04em;"></span></span><span style="top:-3.677em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord">1</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.686em;"><span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mop op-limits"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.6514em;"><span style="top:-1.8723em;margin-left:0em;"><span class="pstrut" style="height:3.05em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">i</span><span class="mrel mtight">=</span><span class="mord mtight">1</span></span></span></span><span style="top:-3.05em;"><span class="pstrut" style="height:3.05em;"></span><span><span class="mop op-symbol large-op">∑</span></span></span><span style="top:-4.3em;margin-left:0em;"><span class="pstrut" style="height:3.05em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">m</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:1.2777em;"><span></span></span></span></span></span><span class="mopen">(</span><span class="mord"><span class="mord mathnormal">h</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3361em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.02778em;">θ</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mopen">(</span><span class="mord"><span class="mord mathnormal">x</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.938em;"><span style="top:-3.113em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mopen mtight">(</span><span class="mord mathnormal mtight">i</span><span class="mclose mtight">)</span></span></span></span></span></span></span></span></span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">−</span><span class="mspace" style="margin-right:0.2222em;"></span></span><span class="base"><span class="strut" style="height:1.188em;vertical-align:-0.25em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.03588em;">y</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.938em;"><span style="top:-3.113em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mopen mtight">(</span><span class="mord mathnormal mtight">i</span><span class="mclose mtight">)</span></span></span></span></span></span></span></span></span><span class="mclose"><span class="mclose">)</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8641em;"><span style="top:-3.113em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span></span></span></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right:1em;"></span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord text"><span class="mord">(</span><span class="mord mathnormal">m</span><span class="mord"> </span><span class="mord cjk_fallback">为样本数</span><span class="mord">)</span></span></span></span></span></span></p>
<ul>
<li>用矩阵的记号写出来</li>
</ul>
<p><span class="katex-display"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><mi>J</mi><mo stretchy="false">(</mo><mi>θ</mi><mo stretchy="false">)</mo><mo>=</mo><mfrac><mn>1</mn><mrow><mn>2</mn><mi>m</mi></mrow></mfrac><mo stretchy="false">(</mo><mi>X</mi><mi>θ</mi><mo>−</mo><mi>y</mi><msup><mo stretchy="false">)</mo><mi>T</mi></msup><mo stretchy="false">(</mo><mi>X</mi><mi>θ</mi><mo>−</mo><mi>y</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">J(\theta)=\frac{1}{2m}(X\theta-y)^T(X\theta-y)
</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathnormal" style="margin-right:0.09618em;">J</span><span class="mopen">(</span><span class="mord mathnormal" style="margin-right:0.02778em;">θ</span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:2.0074em;vertical-align:-0.686em;"></span><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.3214em;"><span style="top:-2.314em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord">2</span><span class="mord mathnormal">m</span></span></span><span style="top:-3.23em;"><span class="pstrut" style="height:3em;"></span><span class="frac-line" style="border-bottom-width:0.04em;"></span></span><span style="top:-3.677em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord">1</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.686em;"><span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span><span class="mopen">(</span><span class="mord mathnormal" style="margin-right:0.02778em;">Xθ</span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">−</span><span class="mspace" style="margin-right:0.2222em;"></span></span><span class="base"><span class="strut" style="height:1.1413em;vertical-align:-0.25em;"></span><span class="mord mathnormal" style="margin-right:0.03588em;">y</span><span class="mclose"><span class="mclose">)</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8913em;"><span style="top:-3.113em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.13889em;">T</span></span></span></span></span></span></span></span><span class="mopen">(</span><span class="mord mathnormal" style="margin-right:0.02778em;">Xθ</span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">−</span><span class="mspace" style="margin-right:0.2222em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathnormal" style="margin-right:0.03588em;">y</span><span class="mclose">)</span></span></span></span></span></p>
<p><img src="https://img.zsaqwq.com/images/2022/03/28/ML_exp1_loss_example.png" alt="loss_function"></p>
</li>
<li>
<p>回归模型的训练过程就是为了得到最好的模型，也就是<strong>让代价函数最小</strong></p>
<ul>
<li>
<p>解析求解（<code>Sklearn</code>里的<code>LinearRegression</code>）</p>
<ul>
<li>可以通过<strong>最小二乘法</strong>得到回归公式中<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>θ</mi></mrow><annotation encoding="application/x-tex">\theta</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6944em;"></span><span class="mord mathnormal" style="margin-right:0.02778em;">θ</span></span></span></span>的解析解</li>
<li>只需要将上面的回归公式对<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>θ</mi></mrow><annotation encoding="application/x-tex">\theta</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6944em;"></span><span class="mord mathnormal" style="margin-right:0.02778em;">θ</span></span></span></span>求导，令其为0，即可解得<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>θ</mi></mrow><annotation encoding="application/x-tex">\theta</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6944em;"></span><span class="mord mathnormal" style="margin-right:0.02778em;">θ</span></span></span></span>的解析解。这个解是：</li>
</ul>
<p><span class="katex-display"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><mi>θ</mi><mo>=</mo><mo stretchy="false">(</mo><msup><mi>X</mi><mi>T</mi></msup><mi>X</mi><msup><mo stretchy="false">)</mo><mrow><mo>−</mo><mn>1</mn></mrow></msup><msup><mi>X</mi><mi>T</mi></msup><mi>y</mi></mrow><annotation encoding="application/x-tex">\theta=(X^TX)^{-1}X^Ty
</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6944em;"></span><span class="mord mathnormal" style="margin-right:0.02778em;">θ</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:1.1413em;vertical-align:-0.25em;"></span><span class="mopen">(</span><span class="mord"><span class="mord mathnormal" style="margin-right:0.07847em;">X</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8913em;"><span style="top:-3.113em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.13889em;">T</span></span></span></span></span></span></span></span><span class="mord mathnormal" style="margin-right:0.07847em;">X</span><span class="mclose"><span class="mclose">)</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8641em;"><span style="top:-3.113em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">−</span><span class="mord mtight">1</span></span></span></span></span></span></span></span></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.07847em;">X</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8913em;"><span style="top:-3.113em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.13889em;">T</span></span></span></span></span></span></span></span><span class="mord mathnormal" style="margin-right:0.03588em;">y</span></span></span></span></span></p>
</li>
<li>
<p>上面的这个解叫做<strong>正规方程（normal equation）</strong></p>
</li>
<li>
<p>解析求解对于大规模的数据有什么问题</p>
<ul>
<li>当样本特征数大的时候，计算<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="false">(</mo><msup><mi>X</mi><mi>T</mi></msup><mi>X</mi><msup><mo stretchy="false">)</mo><mrow><mo>−</mo><mn>1</mn></mrow></msup></mrow><annotation encoding="application/x-tex">(X^TX)^{-1}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.0913em;vertical-align:-0.25em;"></span><span class="mopen">(</span><span class="mord"><span class="mord mathnormal" style="margin-right:0.07847em;">X</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8413em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.13889em;">T</span></span></span></span></span></span></span></span><span class="mord mathnormal" style="margin-right:0.07847em;">X</span><span class="mclose"><span class="mclose">)</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8141em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">−</span><span class="mord mtight">1</span></span></span></span></span></span></span></span></span></span></span></span>十分困难</li>
</ul>
</li>
<li>
<p>我们还可以使用梯度下降的方法得到所需的<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>θ</mi></mrow><annotation encoding="application/x-tex">\theta</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6944em;"></span><span class="mord mathnormal" style="margin-right:0.02778em;">θ</span></span></span></span></p>
</li>
</ul>
</li>
</ul>
<h2 id="3-梯度下降">3. 梯度下降</h2>
<ul>
<li>
<p><em><strong>梯度下降</strong></em></p>
<ul>
<li>沿着<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>J</mi><mo stretchy="false">(</mo><mi>θ</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">J(θ)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathnormal" style="margin-right:0.09618em;">J</span><span class="mopen">(</span><span class="mord mathnormal" style="margin-right:0.02778em;">θ</span><span class="mclose">)</span></span></span></span>的负梯度方向走，我们就能接近其最小值，或者极小值，从而接近更高的预测精度。</li>
<li>重复计算下面的公式，直到收敛：</li>
</ul>
<p><span class="katex-display"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><msub><mi>θ</mi><mrow><mi>j</mi><mo>+</mo><mn>1</mn></mrow></msub><mo>=</mo><msub><mi>θ</mi><mi>j</mi></msub><mo>−</mo><mi>α</mi><mfrac><mi mathvariant="normal">∂</mi><mrow><mi mathvariant="normal">∂</mi><msub><mi>θ</mi><mi>j</mi></msub></mrow></mfrac><mi>J</mi><mo stretchy="false">(</mo><mi>θ</mi><mo stretchy="false">)</mo><mspace width="1em"/><mrow><mstyle scriptlevel="0" displaystyle="false"><mi>α</mi></mstyle><mtext> 为学习率</mtext></mrow></mrow><annotation encoding="application/x-tex">\theta_{j+1} = \theta_j-\alpha\frac{\partial}{\partial\theta_j}J(\theta) \quad \text{$\alpha$ 为学习率}
</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.9805em;vertical-align:-0.2861em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.02778em;">θ</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3117em;"><span style="top:-2.55em;margin-left:-0.0278em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.05724em;">j</span><span class="mbin mtight">+</span><span class="mord mtight">1</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.2861em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:0.9805em;vertical-align:-0.2861em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.02778em;">θ</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3117em;"><span style="top:-2.55em;margin-left:-0.0278em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.05724em;">j</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.2861em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">−</span><span class="mspace" style="margin-right:0.2222em;"></span></span><span class="base"><span class="strut" style="height:2.3435em;vertical-align:-0.9721em;"></span><span class="mord mathnormal" style="margin-right:0.0037em;">α</span><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.3714em;"><span style="top:-2.314em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord" style="margin-right:0.05556em;">∂</span><span class="mord"><span class="mord mathnormal" style="margin-right:0.02778em;">θ</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3117em;"><span style="top:-2.55em;margin-left:-0.0278em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.05724em;">j</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.2861em;"><span></span></span></span></span></span></span></span></span><span style="top:-3.23em;"><span class="pstrut" style="height:3em;"></span><span class="frac-line" style="border-bottom-width:0.04em;"></span></span><span style="top:-3.677em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord" style="margin-right:0.05556em;">∂</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.9721em;"><span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span><span class="mord mathnormal" style="margin-right:0.09618em;">J</span><span class="mopen">(</span><span class="mord mathnormal" style="margin-right:0.02778em;">θ</span><span class="mclose">)</span><span class="mspace" style="margin-right:1em;"></span><span class="mord text"><span class="mord mathnormal" style="margin-right:0.0037em;">α</span><span class="mord"> </span><span class="mord cjk_fallback">为学习率</span></span></span></span></span></span></p>
<ul>
<li>计算<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>J</mi><mo stretchy="false">(</mo><mi>θ</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">J(θ)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathnormal" style="margin-right:0.09618em;">J</span><span class="mopen">(</span><span class="mord mathnormal" style="margin-right:0.02778em;">θ</span><span class="mclose">)</span></span></span></span>的梯度，得：</li>
</ul>
<p><span class="katex-display"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><msub><mi>θ</mi><mrow><mi>j</mi><mo>+</mo><mn>1</mn></mrow></msub><mo>=</mo><msub><mi>θ</mi><mi>j</mi></msub><mo>+</mo><mi>α</mi><mfrac><mn>1</mn><mi>m</mi></mfrac><munderover><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>m</mi></munderover><mo stretchy="false">(</mo><msup><mi>y</mi><mrow><mo stretchy="false">(</mo><mi>i</mi><mo stretchy="false">)</mo></mrow></msup><mo>−</mo><msub><mi>h</mi><mi>θ</mi></msub><mo stretchy="false">(</mo><msup><mi>x</mi><mrow><mo stretchy="false">(</mo><mi>i</mi><mo stretchy="false">)</mo></mrow></msup><mo stretchy="false">)</mo><mo stretchy="false">)</mo><msubsup><mi>x</mi><mi>j</mi><mrow><mo stretchy="false">(</mo><mi>i</mi><mo stretchy="false">)</mo></mrow></msubsup></mrow><annotation encoding="application/x-tex">\theta_{j+1} = \theta_j+\alpha\frac{1}{m}\sum\limits_{i=1}^{m}(y^{(i)}-h_\theta(x^{(i)}))x_j^{(i)}
</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.9805em;vertical-align:-0.2861em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.02778em;">θ</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3117em;"><span style="top:-2.55em;margin-left:-0.0278em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.05724em;">j</span><span class="mbin mtight">+</span><span class="mord mtight">1</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.2861em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:0.9805em;vertical-align:-0.2861em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.02778em;">θ</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3117em;"><span style="top:-2.55em;margin-left:-0.0278em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.05724em;">j</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.2861em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222em;"></span></span><span class="base"><span class="strut" style="height:2.9291em;vertical-align:-1.2777em;"></span><span class="mord mathnormal" style="margin-right:0.0037em;">α</span><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.3214em;"><span style="top:-2.314em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord mathnormal">m</span></span></span><span style="top:-3.23em;"><span class="pstrut" style="height:3em;"></span><span class="frac-line" style="border-bottom-width:0.04em;"></span></span><span style="top:-3.677em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord">1</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.686em;"><span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mop op-limits"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.6514em;"><span style="top:-1.8723em;margin-left:0em;"><span class="pstrut" style="height:3.05em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">i</span><span class="mrel mtight">=</span><span class="mord mtight">1</span></span></span></span><span style="top:-3.05em;"><span class="pstrut" style="height:3.05em;"></span><span><span class="mop op-symbol large-op">∑</span></span></span><span style="top:-4.3em;margin-left:0em;"><span class="pstrut" style="height:3.05em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">m</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:1.2777em;"><span></span></span></span></span></span><span class="mopen">(</span><span class="mord"><span class="mord mathnormal" style="margin-right:0.03588em;">y</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.938em;"><span style="top:-3.113em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mopen mtight">(</span><span class="mord mathnormal mtight">i</span><span class="mclose mtight">)</span></span></span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">−</span><span class="mspace" style="margin-right:0.2222em;"></span></span><span class="base"><span class="strut" style="height:1.4578em;vertical-align:-0.413em;"></span><span class="mord"><span class="mord mathnormal">h</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3361em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.02778em;">θ</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mopen">(</span><span class="mord"><span class="mord mathnormal">x</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.938em;"><span style="top:-3.113em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mopen mtight">(</span><span class="mord mathnormal mtight">i</span><span class="mclose mtight">)</span></span></span></span></span></span></span></span></span><span class="mclose">))</span><span class="mord"><span class="mord mathnormal">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.0448em;"><span style="top:-2.4231em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.05724em;">j</span></span></span><span style="top:-3.2198em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mopen mtight">(</span><span class="mord mathnormal mtight">i</span><span class="mclose mtight">)</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.413em;"><span></span></span></span></span></span></span></span></span></span></span></p>
</li>
<li>
<p><em><strong>收敛</strong></em>：</p>
<ul>
<li>这个沿着负梯度向下的过程一直迭代重复，直到<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>J</mi><mo stretchy="false">(</mo><mi>θ</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">J(θ)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathnormal" style="margin-right:0.09618em;">J</span><span class="mopen">(</span><span class="mord mathnormal" style="margin-right:0.02778em;">θ</span><span class="mclose">)</span></span></span></span>的值基本不变了，就称为收敛。</li>
<li>这时候我们就知道达到了极小值。</li>
</ul>
</li>
<li>
<p><em><strong>随机梯度下降（Stochastic Gradient Descent）</strong></em>（<code>Sklearn</code>里的<code>SGDRegressor</code>）</p>
<ul>
<li>根据上面的公式，每调整一次<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>θ</mi></mrow><annotation encoding="application/x-tex">\theta</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6944em;"></span><span class="mord mathnormal" style="margin-right:0.02778em;">θ</span></span></span></span>，都需要把整个训练数据集都过一遍，效率太低了。</li>
<li>随机梯度下降就是每次迭代时只任选一个样本进行更新，不再对所有样本都计算一遍并求和。</li>
</ul>
\begin{aligned}& \text{重复直到收敛（Repeat until convergence）:} \\

</li>
</ul>
<p>&amp; \quad \quad \theta_j = \theta_j+\alpha(y^{(i)}-h_\theta(x^{(i)}))x_j^{(i)}<br>
\end{aligned}</p>

- 可能会出现抖动，不一定能获得全局最优

- ***学习率（步长）***

- 梯度下降公式中，梯度前面的系数$\alpha$就是学习率，又叫步长。
- 学习率标识了沿梯度方向行进的速率，是一个重要的超参数。
- 如果学习率太大，很可能“迈过”最小值。如果太小，则会收敛得很慢。

<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 4 线性回归预测歌曲的年份</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 用于保存代价函数的历史记录</span></span><br><span class="line">old_stdout = sys.stdout</span><br><span class="line">sys.stdout = mystdout = StringIO()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 模型训练</span></span><br><span class="line"><span class="comment">#################################### Question 4 #############################################</span></span><br><span class="line"><span class="comment"># 要求：</span></span><br><span class="line"><span class="comment">#    - 为lin_reg确定合适的参数，并拟合x_train, y_train进行训练</span></span><br><span class="line"><span class="comment">#    - 使用sklearn.linear_model.SGDRegressor()</span></span><br><span class="line"><span class="comment">#    - 参数verbose应设为1，以遍绘制代价函数的下降</span></span><br><span class="line"><span class="comment">################################ Start of Your Code #########################################</span></span><br><span class="line">lin_reg=linear_model.SGDRegressor(verbose=<span class="number">1</span>,eta0=<span class="number">0.005</span>)</span><br><span class="line">lin_reg.fit(x_train,y_train)</span><br><span class="line"><span class="comment">################################# End of Your Code ##########################################</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 得到代价函数的历史记录</span></span><br><span class="line">sys.stdout = old_stdout</span><br><span class="line">loss_history = mystdout.getvalue()</span><br><span class="line">loss_list = []</span><br><span class="line"><span class="keyword">for</span> line <span class="keyword">in</span> loss_history.split(<span class="string">&#x27;\n&#x27;</span>):</span><br><span class="line">    <span class="keyword">if</span> <span class="built_in">len</span>(line.split(<span class="string">&quot;loss: &quot;</span>)) == <span class="number">1</span>:</span><br><span class="line">        <span class="keyword">continue</span></span><br><span class="line">    loss_list.append(<span class="built_in">float</span>(line.split(<span class="string">&quot;loss: &quot;</span>)[-<span class="number">1</span>]))</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;迭代次数:&#x27;</span>, <span class="built_in">len</span>(loss_list))</span><br><span class="line"></span><br><span class="line"><span class="comment"># 代价函数下降的可视化</span></span><br><span class="line">plt.figure()</span><br><span class="line">plt.plot(np.arange(<span class="built_in">len</span>(loss_list)), loss_list)</span><br><span class="line">plt.xlabel(<span class="string">&quot;Number of epochs&quot;</span>)</span><br><span class="line">plt.ylabel(<span class="string">&quot;Loss&quot;</span>)</span><br></pre></td></tr></table></figure>

迭代次数: 876

Text(0, 0.5, 'Loss')
![png](https://img.zsaqwq.com/images/2022/03/28/ML_exp1_28_2.png)

- 得到训练好的线性回归模型之后，下面评估其预测结果
- 分别在训练集、测试集上评估模型预测值的准确率和均方误差
  - 计算准确率时，不能以完全相等作为相等，应该给定一个范围。
  - 本题中，当预测值与真实值的偏差小于等于5时，认为预测准确。
    - 譬如，若真实年份为2000年，预测年份为1995~2005之间都算作预测准确。
  - ***均方误差（Mean square error, MSE）***也可以作为评价标准
- 然后从训练集、测试集上分别抽取一些样本，直观对比的它们真实值和预测值

<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">################################## Question 5 ########################################</span></span><br><span class="line"><span class="comment"># 要求：完成函数evaluate_model()，使其具备下面的功能。</span></span><br><span class="line"><span class="comment">#   - 功能：得到一个模型在一个数据集上的准确率和均方差</span></span><br><span class="line"><span class="comment">#     - 注：本题中，对某一样本，当预测值与实际值相差不大于5时，认为预测准确。</span></span><br><span class="line"><span class="comment">#   - 参数：</span></span><br><span class="line"><span class="comment">#     - model：要评估的模型</span></span><br><span class="line"><span class="comment">#     - X: 数据集的特征</span></span><br><span class="line"><span class="comment">#     - y：数据集的标签</span></span><br><span class="line"><span class="comment">#   - 返回值：</span></span><br><span class="line"><span class="comment">#     - 返回值1：准确率</span></span><br><span class="line"><span class="comment">#     - 返回值2：均方误差</span></span><br><span class="line"><span class="comment"># 提示：</span></span><br><span class="line"><span class="comment">#   - 准确率的计算可以借助sklearn.metrics.accuracy_score()方法</span></span><br><span class="line"><span class="comment">#   - 均方差的计算可以借助sklearn.metrics.mean_squared_error()方法</span></span><br><span class="line"><span class="comment">#   - 上面提到的两种方法只是一种可能的实现，并不是一定要用到</span></span><br><span class="line"><span class="comment">############################## Start of Your Code #####################################</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">evaluate_model</span>(<span class="params">model,x,y</span>):</span><br><span class="line">    y_pred = model.predict(x)</span><br><span class="line">    y_true = np.array(y)</span><br><span class="line">    diff = <span class="built_in">abs</span>(y_pred-y_true)</span><br><span class="line">    correct_num = diff[diff&lt;=<span class="number">5</span>].size</span><br><span class="line">    accuracy = correct_num/y_true.size</span><br><span class="line">    mse = metrics.mean_squared_error(y_true,y_pred)</span><br><span class="line">    <span class="keyword">return</span> accuracy,mse</span><br><span class="line"></span><br><span class="line"><span class="comment">################################ End of Your Code #####################################</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 训练集准确率和均方差</span></span><br><span class="line">accuracy_rate_lr_train, mse_lr_train = evaluate_model(lin_reg, x_train, y_train)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;模型训练准确率为:&#x27;</span>, accuracy_rate_lr_train)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;模型训练均方误差为:&#x27;</span>, mse_lr_train)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 测试集准确率和均方差</span></span><br><span class="line">accuracy_rate_lr_test, mse_lr_test = evaluate_model(lin_reg, x_test, y_test)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;模型测试准确率为:&#x27;</span>, accuracy_rate_lr_test)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;模型测试均方误差为:&#x27;</span>, mse_lr_test)</span><br></pre></td></tr></table></figure>

> 模型训练准确率为: 0.45917427730394744
> 模型训练均方误差为: 101.69800987928029
> 模型测试准确率为: 0.4543869843114468
> 模型测试均方误差为: 100.94908156645756

<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">barplot</span>(<span class="params">true, pred, yi=<span class="literal">None</span>, ym=<span class="literal">None</span></span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    用于绘制柱状图以对比真实值和预测值的函数</span></span><br><span class="line"><span class="string">    yi: 纵坐标的下限</span></span><br><span class="line"><span class="string">    ym: 纵坐标的上限</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    width=<span class="number">0.3</span>;</span><br><span class="line">    x = np.array([x <span class="keyword">for</span> x <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">1</span>,<span class="built_in">len</span>(true)+<span class="number">1</span>)]); <span class="comment"># 第一个数据序列x轴</span></span><br><span class="line">    x1 = x - width; <span class="comment"># 为使其并列，使得第一个x轴序列全部减去条宽度</span></span><br><span class="line">    plt.bar(x1,true,width=width, label=<span class="string">&#x27;true&#x27;</span>);</span><br><span class="line">    plt.bar(x,pred,width=width, label=<span class="string">&#x27;predicted&#x27;</span>);</span><br><span class="line">    plt.legend(loc=<span class="string">&#x27;upper left&#x27;</span>); <span class="comment"># 设置标签显示在左上角</span></span><br><span class="line">    <span class="keyword">if</span> yi <span class="keyword">and</span> ym:</span><br><span class="line">        plt.ylim(yi,ym);</span><br><span class="line">    plt.show();</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 针对训练数据集，对比预测值和实际值</span></span><br><span class="line">samples_x_train, samples_y_train = shuffle(x_train, y_train, n_samples=<span class="number">10</span>, random_state=<span class="number">0</span>)  <span class="comment"># 从训练集中随机选取10个样本</span></span><br><span class="line">predicted_train = lin_reg.predict(samples_x_train)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;训练集样本的原始标签:\n&#x27;</span>, samples_y_train.values)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;训练集样本的预测结果:\n&#x27;</span>, predicted_train)</span><br><span class="line">barplot(samples_y_train, predicted_train, <span class="number">1900</span>, <span class="number">2030</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;-------------------------------------------&#x27;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 针对测试数据集，展示预测效果</span></span><br><span class="line">samples_x_test, samples_y_test = shuffle(x_test, y_test, n_samples=<span class="number">10</span>, random_state=<span class="number">0</span>)</span><br><span class="line">predicted_test = lin_reg.predict(samples_x_test)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;测试集样本的原始标签:&#x27;</span>, samples_y_test.values)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;测试集样本的预测结果:&#x27;</span>, predicted_test)</span><br><span class="line">barplot(samples_y_test, predicted_test, <span class="number">1900</span>, <span class="number">2030</span>)</span><br></pre></td></tr></table></figure>

> 训练集样本的原始标签:
> [2006 1992 1994 2007 1966 2000 2001 1996 1999 2009]
> 训练集样本的预测结果:
> [2001.58127171 1993.26745612 1999.75829637 2005.59255932 1992.13467085
> 1998.29617644 1995.8651914  1999.77619584 1998.80305912 2004.77065429]
> ![png](https://img.zsaqwq.com/images/2022/03/28/ML_exp1_31_1.png)

---

> 测试集样本的原始标签: [2004 2008 1940 1981 2007 1979 2005 1997 2004 2010]
> 测试集样本的预测结果: [1996.26416028 2001.55710336 1990.09362799 1993.38272902 2004.83357094
> 1990.53759828 2002.69854745 2000.56446419 2001.9340644  1980.2019072 ]
> ![png](https://img.zsaqwq.com/images/2022/03/28/ML_exp1_31_3.png)

### 对比模型对于训练数据集和测试数据集的预测结果

- 可以看到，模型对测试集的预测效果往往不如训练集的效果。
- 如果训练集训练出来的模型也能很好地预测测试集的数据，那么这个模型的**泛化**能力就好。

# 五. 多项式回归

## 1. 多项式回归介绍

- **多项式回归**

- 不再使用单纯的线性函数去拟合数据，而是使用一个多项式函数。
- 例如，一个二次多项式：


<p>P(x)=\theta^T[1,x_1,x_2,x_1^2,x_2^2]</p>

- 会拓展数据集特征空间的维度

- 当线性回归效果不好时，即***欠拟合***时，需要尝试多项式回归

- 欠拟合：当模型在训练集中表现不好时，自然更不会在测试数据中表现得好。这种情况就是欠拟合。

## 2. 交互项

- 多项式回归中可以增加***交互项***
- 什么是交互项
- 例如，在二次多项式$P(x)=\theta^T[1,x_1,x_2,x_1^2,x_1x_2,x_2^2]$中，$x_1x_2$就是交叉项
- 以本课问题为例，如果某一音乐家在某一年代倾向于创作较长的音乐，另一时间段倾向于创作较短的音乐。这种情况下，可以增加创作者这一特征和音乐时长这一特征的交互项，帮助模型拟合
- 我们这里并没有这么做，因为选取的特征并不符合这一特征

<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 5 使用多项式回归训练模型</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 多项式数据准备</span></span><br><span class="line"><span class="comment">#################################### Question 6 #############################################</span></span><br><span class="line"><span class="comment"># 要求：</span></span><br><span class="line"><span class="comment">#   - 将X中的数据转化成三次多项式的数据</span></span><br><span class="line"><span class="comment">#   - 再将转化好的数据进行标准化、数据划分，得到poly_x_train, poly_x_test，类型：DataFrame</span></span><br><span class="line"><span class="comment">#      - 数据划分的比例仍应该按照Question 3的要求</span></span><br><span class="line"><span class="comment"># 提示：</span></span><br><span class="line"><span class="comment">#   - 使用sklearn.preprocessing.PolynomialFeatures()进行转化</span></span><br><span class="line"><span class="comment">#   - 如果计算机的性能很差，可以只使用二次多项式（一般来说都是可以的）</span></span><br><span class="line"><span class="comment">################################ Start of Your Code #########################################</span></span><br><span class="line">poly = preprocessing.PolynomialFeatures(degree=<span class="number">3</span>,include_bias=<span class="literal">True</span>)</span><br><span class="line">X_trib = poly.fit_transform(X)</span><br><span class="line">scaler_trib = preprocessing.MinMaxScaler()</span><br><span class="line">X_trib_scaled = pd.DataFrame(scaler_trib.fit_transform(X_trib))</span><br><span class="line">poly_x_train=X_trib_scaled[:<span class="number">463715</span>]</span><br><span class="line">poly_x_test=X_trib_scaled[<span class="number">463715</span>:]</span><br><span class="line"></span><br><span class="line"><span class="comment">################################# End of Your Code ##########################################</span></span><br><span class="line"><span class="built_in">print</span>(poly_x_train.shape)</span><br><span class="line"><span class="built_in">print</span>(poly_x_test.shape)</span><br></pre></td></tr></table></figure>

> (463715, 455)
> (51630, 455)

<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 用于保存代价函数的历史记录</span></span><br><span class="line">old_stdout = sys.stdout</span><br><span class="line">sys.stdout = mystdout = StringIO()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 模型训练</span></span><br><span class="line"><span class="comment">#################################### Question 7 #############################################</span></span><br><span class="line"><span class="comment"># 要求：</span></span><br><span class="line"><span class="comment">#    - 为pol_reg确定合适的参数，并拟合poly_x_train, y_train进行训练</span></span><br><span class="line"><span class="comment">#    - 使用sklearn.linear_model.SGDRegressor()</span></span><br><span class="line"><span class="comment">#    - 参数verbose应设为1，以遍绘制代价函数的下降</span></span><br><span class="line"><span class="comment">################################ Start of Your Code #########################################</span></span><br><span class="line">pol_reg=linear_model.SGDRegressor(verbose=<span class="number">1</span>,eta0=<span class="number">0.01</span>,tol=<span class="literal">None</span>,max_iter=<span class="number">1000</span>)</span><br><span class="line">pol_reg.fit(poly_x_train,y_train)</span><br><span class="line"><span class="comment">################################# End of Your Code ##########################################</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 得到代价函数的历史记录</span></span><br><span class="line">sys.stdout = old_stdout</span><br><span class="line">loss_history = mystdout.getvalue()</span><br><span class="line">loss_list = []</span><br><span class="line"><span class="keyword">for</span> line <span class="keyword">in</span> loss_history.split(<span class="string">&#x27;\n&#x27;</span>):</span><br><span class="line">    <span class="keyword">if</span>(<span class="built_in">len</span>(line.split(<span class="string">&quot;loss: &quot;</span>)) == <span class="number">1</span>):</span><br><span class="line">        <span class="keyword">continue</span></span><br><span class="line">    loss_list.append(<span class="built_in">float</span>(line.split(<span class="string">&quot;loss: &quot;</span>)[-<span class="number">1</span>]))</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="built_in">len</span>(loss_list))</span><br><span class="line"></span><br><span class="line"><span class="comment"># 代价函数下降的可视化</span></span><br><span class="line">plt.figure()</span><br><span class="line">plt.plot(np.arange(<span class="built_in">len</span>(loss_list)), loss_list)</span><br><span class="line">plt.xlabel(<span class="string">&quot;Time in epochs&quot;</span>)</span><br><span class="line">plt.ylabel(<span class="string">&quot;Loss&quot;</span>)</span><br></pre></td></tr></table></figure>

> 1000

> Text(0, 0.5, 'Loss')
> ![png](https://img.zsaqwq.com/images/2022/03/28/ML_exp1_35_2.png)

<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 训练集准确率和均方差</span></span><br><span class="line">accuracy_rate_pr_train, mse_pr_train = evaluate_model(pol_reg, poly_x_train, y_train)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;模型训练准确率为:&#x27;</span>, accuracy_rate_pr_train)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;模型训练均方误差为:&#x27;</span>, mse_pr_train)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 测试集准确率和均方差</span></span><br><span class="line">accuracy_rate_pr_test, mse_pr_test = evaluate_model(pol_reg, poly_x_test, y_test)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;模型测试准确率为:&#x27;</span>, accuracy_rate_pr_test)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;模型测试均方误差为:&#x27;</span>, mse_pr_test)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;-------------------------------------------&#x27;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 针对训练数据集，展示预测效果</span></span><br><span class="line">samples_poly_x_train, samples_y_train = shuffle(poly_x_train, y_train, n_samples=<span class="number">10</span>, random_state=<span class="number">0</span>)</span><br><span class="line">poly_predicted_train = pol_reg.predict(samples_poly_x_train)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;训练集样本的原始标签:&#x27;</span>, samples_y_train.values)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;训练集样本的预测结果:&#x27;</span>, poly_predicted_train)</span><br><span class="line">barplot(samples_y_train, poly_predicted_train, <span class="number">1900</span>, <span class="number">2030</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;-------------------------------------------&#x27;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 针对测试数据集，展示预测效果</span></span><br><span class="line">samples_poly_x_test, samples_y_test = shuffle(poly_x_test, y_test, n_samples=<span class="number">10</span>, random_state=<span class="number">0</span>)</span><br><span class="line">poly_predicted_test = pol_reg.predict(samples_poly_x_test)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;测试集样本的原始标签:&#x27;</span>, samples_y_test.values)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;测试集样本的预测结果:&#x27;</span>, poly_predicted_test)</span><br><span class="line">barplot(samples_y_test, poly_predicted_test, <span class="number">1900</span>, <span class="number">2030</span>)</span><br></pre></td></tr></table></figure>

> 模型训练准确率为: 0.41047626235942336
> 模型训练均方误差为: 99.71850133606817
> 模型测试准确率为: 0.4090451288010846
> 模型测试均方误差为: 100.73636768027744模型训练准确率为: 0.41047626235942336
> 模型训练均方误差为: 99.71850133606817
> 模型测试准确率为: 0.4090451288010846
> 模型测试均方误差为: 100.73636768027744

> 训练集样本的原始标签: [2006 1992 1994 2007 1966 2000 2001 1996 1999 2009]
> 训练集样本的预测结果: [2000.61202988 1989.1528712  1999.36643953 2005.43103627 1987.8140294
> 1995.44083552 1993.03184727 1996.60640708 1999.09366353 2002.60016725]
> ![png](https://img.zsaqwq.com/images/2022/03/28/ML_exp1_36_1.png)

---

> 测试集样本的原始标签: [2004 2008 1940 1981 2007 1979 2005 1997 2004 2010]
> 测试集样本的预测结果: [1993.33772763 1999.5744086  1986.88562316 1992.77738124 2006.41072662
> 1986.45687003 2001.59073335 1999.02043011 1999.11527887 1991.54463823]
> ![png](https://img.zsaqwq.com/images/2022/03/28/ML_exp1_36_3.png)

- 使用多项式回归可以解决欠拟合的问题，但又可能带来过拟合
- ***过拟合***：指模型可以非常好地拟合训练数据，预测测试集时却表现很差

### 理解欠拟合与过拟合

例如我们想要拟合下面的数据
<img src="https://img.zsaqwq.com/images/2022/03/28/ML_exo1_data.png" width="400" hegiht="300" align=center />

- 如果采用线性回归，那么拟合效果显然不好，数据距离拟合曲线较远
- 采用二次多项式回归，拟合效果刚刚好
- 采用100次多项式回归，虽然貌似拟合几乎每一个数据，但是丢失了信息规律，显然不能很好地预测未知的数据

![underfitvsoverfit](https://img.zsaqwq.com/images/2022/03/28/ML_exp1_underoverfit.png)

# 六. 岭回归

## 1. 正则化

- 正如上面过拟合的例子显示，当模型**复杂度**高（比如参数很多）的时候，很容易出现过拟合。
- 训练集误差很低，测试集表现较差——模型的泛化能力较差。
- ***正则化(Regularization)*** 是一种减少**测试**误差的手段。
- 我们试图拉伸函数曲线使之更加平滑
- 为了拉伸曲线，也就要弱化一些高阶项（曲线曲折的罪魁祸首）
- 为此，我们想要降低高阶项的系数$\theta_i$大小——这叫做**惩罚**
- 做法：
- 设模型的代价函数是$L(\theta)$，而我们的惩罚项是$R(\theta)$
- 那么，将目标函数写为$J(\theta)=L(\theta)+\alpha R(\theta)$，再最小化目标函数即可
- 其中$\alpha$是一个超参数，它决定了正则化惩罚的“力度”。$\alpha$越大，惩罚“力度”越大。

## 2. 岭回归

- ***岭回归***是一种常用的正则化线性回归。

- 使用线性回归的代价函数，而惩罚项$R(\theta)=\sum\limits_{i=1}^{n}\theta_i^2$
- 也就是目标函数写为：


<p>J(\theta)=L(\theta)+\alpha\sum\limits_{i=1}^{n}\theta_i^2</p>

- 惩罚项的含义是空间中参数点$\theta$到原点的**欧式距离**的平方

- 因此惩罚的效果是迫使参数点靠近原点，也就是迫使参数变小。

- 可以降低模型的复杂度，提高模型的泛化能力，即模型适用于新的数据集的能力

<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 使用岭回归训练模型</span></span><br><span class="line"><span class="comment"># 模型训练</span></span><br><span class="line"><span class="comment">#################################### Question 8 #############################################</span></span><br><span class="line"><span class="comment"># 要求：</span></span><br><span class="line"><span class="comment">#   - 确定ridge_reg的参数，使用sklearn.linear_model.Ridge()</span></span><br><span class="line"><span class="comment">#   - 用多项式数据，即poly_x_train, y_train来训练ridge_reg</span></span><br><span class="line"><span class="comment">################################ Start of Your Code #########################################</span></span><br><span class="line">ridge_reg = sklearn.linear_model.Ridge(alpha=<span class="number">1.0</span>)</span><br><span class="line">ridge_reg.fit(poly_x_train,y_train)</span><br><span class="line"><span class="comment">################################# End of Your Code ##########################################</span></span><br></pre></td></tr></table></figure>

Ridge()

- 预测数据是否会符合我们预期，波动变小呢？

<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 训练集准确率和均方差</span></span><br><span class="line">accuracy_rate_rr_train, mse_rr_train = evaluate_model(ridge_reg, poly_x_train, y_train)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;模型训练准确率为:&#x27;</span>, accuracy_rate_rr_train)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;模型训练均方差为:&#x27;</span>, mse_rr_train)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;-------------------------------------------&#x27;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 测试集准确率和均方差</span></span><br><span class="line">accuracy_rate_rr_test, mse_rr_test = evaluate_model(ridge_reg, poly_x_test, y_test)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;模型测试准确率为:&#x27;</span>, accuracy_rate_rr_test)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;模型测试均方差为:&#x27;</span>, mse_rr_test)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;-------------------------------------------&#x27;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 针对训练数据集，展示预测效果</span></span><br><span class="line">samples_x_train, samples_y_train = shuffle(poly_x_train, y_train, n_samples=<span class="number">10</span>, random_state=<span class="number">0</span>)</span><br><span class="line">predicted_train = ridge_reg.predict(samples_x_train)  <span class="comment"># 做预测</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;训练集样本的原始标签:&#x27;</span>, samples_y_train.values)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;训练集样本的预测结果:&#x27;</span>, predicted_train)</span><br><span class="line">barplot(samples_y_train, predicted_train, <span class="number">1900</span>, <span class="number">2030</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;-------------------------------------------&#x27;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 4.3.2 针对测试数据集，展示预测效果</span></span><br><span class="line">samples_x_test, samples_y_test = shuffle(poly_x_test, y_test, n_samples=<span class="number">10</span>, random_state=<span class="number">0</span>)</span><br><span class="line">predicted_test = ridge_reg.predict(samples_x_test)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;测试集样本的原始标签:&#x27;</span>, samples_y_test.values)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;测试集样本的预测结果:&#x27;</span>, predicted_test)</span><br><span class="line">barplot(samples_y_test, predicted_test, <span class="number">1900</span>, <span class="number">2030</span>)</span><br></pre></td></tr></table></figure>

> 模型训练准确率为: 0.4958843255016551
> 模型训练均方差为: 93.07491749727983模型训练准确率为: 0.4958843255016551
> 模型训练均方差为: 93.07491749727983

> 模型测试准确率为: 0.4905481309316289
> 模型测试均方差为: 93.51400717023266模型测试准确率为: 0.4905481309316289
> 模型测试均方差为: 93.51400717023266

> 训练集样本的原始标签: [2006 1992 1994 2007 1966 2000 2001 1996 1999 2009]
> 训练集样本的预测结果: [2002.73643893 1991.12225283 2001.42164601 2007.23058105 1987.80480768
> 1997.97548257 1994.13579631 1998.32564721 2002.88598941 2006.32989283]
> ![png](https://img.zsaqwq.com/images/2022/03/28/ML_exp1_41_1.png)

---

> 测试集样本的原始标签: [2004 2008 1940 1981 2007 1979 2005 1997 2004 2010]
> 测试集样本的预测结果: [1995.27325062 2002.00472514 1991.56437322 1995.51972086 2009.06087812
> 1986.12915134 2004.48084521 2000.76110235 1999.82108022 1996.69017132]
> ![png](https://img.zsaqwq.com/images/2022/03/28/ML_exp1_41_3.png)

### 线性回归、多项式回归、岭回归模型的表现差异

- 对比三个模型对于测试集的预测准确率、均方差

<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">data_present=[[<span class="string">&#x27;线性回归&#x27;</span>, <span class="string">&#x27;训练集&#x27;</span>, accuracy_rate_lr_train, mse_lr_train],</span><br><span class="line">    [<span class="string">&#x27;线性回归&#x27;</span>, <span class="string">&#x27;测试集&#x27;</span>, accuracy_rate_lr_test, mse_lr_test],</span><br><span class="line">    [<span class="string">&#x27;多项式回归&#x27;</span>, <span class="string">&#x27;训练集&#x27;</span>, accuracy_rate_pr_train, mse_pr_train],</span><br><span class="line">    [<span class="string">&#x27;多项式回归&#x27;</span>, <span class="string">&#x27;测试集&#x27;</span>, accuracy_rate_pr_test, mse_pr_test],</span><br><span class="line">    [<span class="string">&#x27;岭回归&#x27;</span>, <span class="string">&#x27;训练集&#x27;</span>, accuracy_rate_rr_train, mse_rr_train],</span><br><span class="line">    [<span class="string">&#x27;岭回归&#x27;</span>, <span class="string">&#x27;测试集&#x27;</span>, accuracy_rate_rr_test, mse_rr_test]]</span><br><span class="line"></span><br><span class="line"><span class="comment"># 使用PrettyTable做一个表格</span></span><br><span class="line">table = pt.PrettyTable()</span><br><span class="line">table.field_names = [<span class="string">&quot;模型&quot;</span>, <span class="string">&quot;数据集&quot;</span>, <span class="string">&quot;准确率&quot;</span>, <span class="string">&quot;均方差&quot;</span>]</span><br><span class="line"><span class="keyword">for</span> row <span class="keyword">in</span> data_present:</span><br><span class="line">    table.add_row(row)</span><br><span class="line"><span class="built_in">print</span>(table)</span><br></pre></td></tr></table></figure>

<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">|    模型    | 数据集 |        准确率       |       均方差       |</span><br><span class="line">+------------+--------+---------------------+--------------------+</span><br><span class="line">|  线性回归  | 训练集 | 0.45917427730394744 | 101.69800987928029 |</span><br><span class="line">|  线性回归  | 测试集 |  0.4543869843114468 | 100.94908156645756 |</span><br><span class="line">| 多项式回归 | 训练集 | 0.41047626235942336 | 99.71850133606817  |</span><br><span class="line">| 多项式回归 | 测试集 |  0.4090451288010846 | 100.73636768027744 |</span><br><span class="line">|   岭回归   | 训练集 |  0.4958843255016551 | 93.07491749727983  |</span><br><span class="line">|   岭回归   | 测试集 |  0.4905481309316289 | 93.51400717023266  |</span><br><span class="line">+------------+--------+---------------------+--------------------+</span><br></pre></td></tr></table></figure>

</article><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta">文章作者: </span><span class="post-copyright-info"><a href="mailto:GearyZhang@outlook.com">TideDra</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta">文章链接: </span><span class="post-copyright-info"><a href="https://tidedra.top/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/ML-exp1/">https://tidedra.top/机器学习/ML-exp1/</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta">版权声明: </span><span class="post-copyright-info">本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" target="_blank">CC BY-NC-SA 4.0</a> 许可协议。转载请注明来自 <a href="https://tidedra.top" target="_blank">泷汐の精神时光屋</a>！</span></div></div><div class="tag_share"><div class="post-meta__tag-list"><a class="post-meta__tags" href="/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/">机器学习</a></div><div class="post_share"><div class="social-share" data-image="https://img.zsaqwq.com/images/2022/03/28/ML_exp1_cover.jpg" data-sites="facebook,twitter,wechat,weibo,qq"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/social-share.js/dist/css/share.min.css" media="print" onload="this.media='all'"><script src="https://cdn.jsdelivr.net/npm/social-share.js/dist/js/social-share.min.js" defer></script></div></div><div class="post-reward"><div class="reward-button"><i class="fas fa-qrcode"></i> 打赏</div><div class="reward-main"><ul class="reward-all"><li class="reward-item"><a href="/img/wechat_reward.jpg" target="_blank"><img class="post-qr-code-img" src="/img/wechat_reward.jpg" alt="微信打赏"/></a><div class="post-qr-code-desc">微信打赏</div></li><li class="reward-item"><a href="/img/alipay_reward.jpg" target="_blank"><img class="post-qr-code-img" src="/img/alipay_reward.jpg" alt="支付宝付款"/></a><div class="post-qr-code-desc">支付宝付款</div></li></ul></div></div><nav class="pagination-post" id="pagination"><div class="prev-post pull-left"><a href="/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/VM-performance-test/"><img class="prev-cover" src="https://img.zsaqwq.com/images/2022/03/27/wsl2_logo.png" onerror="onerror=null;src='/img/404.jpg'" alt="cover of previous post"><div class="pagination-info"><div class="label">上一篇</div><div class="prev_info">【操作系统】虚拟机（WSL2）的性能相比真机缩水了多少？</div></div></a></div><div class="next-post pull-right"><a href="/%E8%BD%AF%E4%BB%B6%E6%8E%A8%E8%8D%90/md-recommand/"><img class="next-cover" src="https://img.zsaqwq.com/images/2022/03/25/default_top.png" onerror="onerror=null;src='/img/404.jpg'" alt="cover of next post"><div class="pagination-info"><div class="label">下一篇</div><div class="next_info">md_recommand</div></div></a></div></nav><hr/><div id="post-comment"><div class="comment-head"><div class="comment-headline"><i class="fas fa-comments fa-fw"></i><span> 评论</span></div></div><div class="comment-wrap"><div><div class="vcomment" id="vcomment"></div></div></div></div></div><div class="aside-content" id="aside-content"><div class="card-widget card-info"><div class="is-center"><div class="avatar-img"><img src="https://img.zsaqwq.com/images/2022/03/26/girl.jpg" onerror="this.onerror=null;this.src='/img/friend_404.gif'" alt="avatar"/></div><div class="author-info__name">TideDra</div><div class="author-info__description">东南大学吴健雄学院在读本科生</div></div><div class="card-info-data is-center"><div class="card-info-data-item"><a href="/archives/"><div class="headline">文章</div><div class="length-num">9</div></a></div><div class="card-info-data-item"><a href="/tags/"><div class="headline">标签</div><div class="length-num">11</div></a></div><div class="card-info-data-item"><a href="/categories/"><div class="headline">分类</div><div class="length-num">6</div></a></div></div><a id="card-info-btn" target="_blank" rel="noopener" href="https://github.com/TideDra"><i class="fab fa-github"></i><span>Follow Me</span></a><div class="card-info-social-icons is-center"><a class="social-icon" href="https://github.com/TideDra" target="_blank" title="Github"><i class="fab fa-github"></i></a><a class="social-icon" href="mailto:GearyZhang@outlook.com" target="_blank" title="Email"><i class="fas fa-envelope"></i></a><a class="social-icon" href="tencent://message/?uin=718525108&amp;Site=&amp;Menu=yes" target="_blank" title="QQ"><i class="fa-brands fa-qq"></i></a></div></div><div class="card-widget card-announcement"><div class="item-headline"><i class="fas fa-bullhorn fa-shake"></i><span>公告</span></div><div class="announcement_content">欢迎来到我的博客！<br />东南大学超算平台上线啦！欢迎点击:<a target="_blank" rel="noopener" href="https://cswu-challenge.github.io/"><input type=button value="东南大学超算平台"></a></div></div><div class="sticky_layout"><div class="card-widget" id="card-toc"><div class="item-headline"><i class="fas fa-stream"></i><span>目录</span><span class="toc-percentage"></span></div><div class="toc-content"><ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link"><span class="toc-number">1.</span> <span class="toc-text">前言</span></a></li><li class="toc-item toc-level-1"><a class="toc-link"><span class="toc-number">2.</span> <span class="toc-text">线性回归——音乐年代预测</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E7%9B%AE%E5%BD%95"><span class="toc-number">2.1.</span> <span class="toc-text">目录</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link"><span class="toc-number">3.</span> <span class="toc-text">一. 概述：音乐年代预测与回归问题</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#1-%E9%9F%B3%E4%B9%90%E5%B9%B4%E4%BB%A3%E9%A2%84%E6%B5%8B%E9%97%AE%E9%A2%98"><span class="toc-number">3.1.</span> <span class="toc-text">1. 音乐年代预测问题</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#2-%E5%9B%9E%E5%BD%92%E9%97%AE%E9%A2%98%EF%BC%88regression%EF%BC%89"><span class="toc-number">3.2.</span> <span class="toc-text">2. 回归问题（regression）</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link"><span class="toc-number">4.</span> <span class="toc-text">二. 数据集的加载与展示</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#1-%E7%99%BE%E4%B8%87%E6%AD%8C%E6%9B%B2%E6%95%B0%E6%8D%AE%E9%9B%86"><span class="toc-number">4.1.</span> <span class="toc-text">1. 百万歌曲数据集</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#2-%E6%95%B0%E6%8D%AE%E9%9B%86%E7%9A%84%E5%8A%A0%E8%BD%BD"><span class="toc-number">4.2.</span> <span class="toc-text">2. 数据集的加载</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#3-%E6%95%B0%E6%8D%AE%E9%9B%86%E7%9A%84%E5%B1%95%E7%A4%BA"><span class="toc-number">4.3.</span> <span class="toc-text">3. 数据集的展示</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link"><span class="toc-number">5.</span> <span class="toc-text">三. 特征筛选与预处理</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#1-%E7%89%B9%E5%BE%81%E7%AD%9B%E9%80%89"><span class="toc-number">5.1.</span> <span class="toc-text">1. 特征筛选</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#2-%E7%89%B9%E5%BE%81%E7%9A%84%E6%95%B0%E5%80%BC%E5%88%86%E5%B8%83%E5%B1%95%E7%A4%BA"><span class="toc-number">5.2.</span> <span class="toc-text">2. 特征的数值分布展示</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#3-%E7%89%B9%E5%BE%81%E7%BC%A9%E6%94%BE"><span class="toc-number">5.3.</span> <span class="toc-text">3. 特征缩放</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#4-%E7%89%B9%E5%BE%81%E5%B1%95%E7%A4%BA"><span class="toc-number">5.4.</span> <span class="toc-text">4. 特征展示</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#5-%E6%95%B0%E6%8D%AE%E9%9B%86%E5%88%92%E5%88%86"><span class="toc-number">5.5.</span> <span class="toc-text">5. 数据集划分</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link"><span class="toc-number">6.</span> <span class="toc-text">四. 线性回归</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#1-%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92"><span class="toc-number">6.1.</span> <span class="toc-text">1. 线性回归</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#2-%E4%BB%A3%E4%BB%B7%E5%87%BD%E6%95%B0"><span class="toc-number">6.2.</span> <span class="toc-text">2. 代价函数</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#3-%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D"><span class="toc-number">6.3.</span> <span class="toc-text">3. 梯度下降</span></a></li></ol></li></ol></div></div><div class="card-widget card-recent-post"><div class="item-headline"><i class="fas fa-history"></i><span>最新文章</span></div><div class="aside-list"><div class="aside-list-item"><a class="thumbnail" href="/%E8%BD%AF%E4%BB%B6%E6%8E%A8%E8%8D%90/md-recommand/" title="md_recommand"><img src="https://img.zsaqwq.com/images/2022/03/25/default_top.png" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="md_recommand"/></a><div class="content"><a class="title" href="/%E8%BD%AF%E4%BB%B6%E6%8E%A8%E8%8D%90/md-recommand/" title="md_recommand">md_recommand</a><time datetime="2022-03-28T13:46:33.000Z" title="发表于 2022-03-28 21:46:33">2022-03-28</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/ML-exp1/" title="【机器学习】入门ML？跟着做这几个实验足矣！（一）"><img src="https://img.zsaqwq.com/images/2022/03/28/ML_exp1_cover.jpg" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="【机器学习】入门ML？跟着做这几个实验足矣！（一）"/></a><div class="content"><a class="title" href="/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/ML-exp1/" title="【机器学习】入门ML？跟着做这几个实验足矣！（一）">【机器学习】入门ML？跟着做这几个实验足矣！（一）</a><time datetime="2022-03-28T07:48:41.000Z" title="发表于 2022-03-28 15:48:41">2022-03-28</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/VM-performance-test/" title="【操作系统】虚拟机（WSL2）的性能相比真机缩水了多少？"><img src="https://img.zsaqwq.com/images/2022/03/27/wsl2_logo.png" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="【操作系统】虚拟机（WSL2）的性能相比真机缩水了多少？"/></a><div class="content"><a class="title" href="/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/VM-performance-test/" title="【操作系统】虚拟机（WSL2）的性能相比真机缩水了多少？">【操作系统】虚拟机（WSL2）的性能相比真机缩水了多少？</a><time datetime="2022-03-27T07:00:20.000Z" title="发表于 2022-03-27 15:00:20">2022-03-27</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/ASC%E8%B6%85%E7%AE%97%E7%AB%9E%E8%B5%9B%E6%95%99%E7%A8%8B/Tensorflow/tensorflow-opt/" title="【ASC超算教程】Tensorflow模型优化思路"><img src="https://img.zsaqwq.com/images/2022/03/26/tensorflow_logo.jpg" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="【ASC超算教程】Tensorflow模型优化思路"/></a><div class="content"><a class="title" href="/ASC%E8%B6%85%E7%AE%97%E7%AB%9E%E8%B5%9B%E6%95%99%E7%A8%8B/Tensorflow/tensorflow-opt/" title="【ASC超算教程】Tensorflow模型优化思路">【ASC超算教程】Tensorflow模型优化思路</a><time datetime="2022-03-25T17:44:42.000Z" title="发表于 2022-03-26 01:44:42">2022-03-26</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/ASC%E8%B6%85%E7%AE%97%E7%AB%9E%E8%B5%9B%E6%95%99%E7%A8%8B/ASC-introduction/" title="【ASC超算教程】ASC竞赛快速入门"><img src="https://img.zsaqwq.com/images/2022/03/26/ASC_logo.png" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="【ASC超算教程】ASC竞赛快速入门"/></a><div class="content"><a class="title" href="/ASC%E8%B6%85%E7%AE%97%E7%AB%9E%E8%B5%9B%E6%95%99%E7%A8%8B/ASC-introduction/" title="【ASC超算教程】ASC竞赛快速入门">【ASC超算教程】ASC竞赛快速入门</a><time datetime="2022-03-25T17:43:42.000Z" title="发表于 2022-03-26 01:43:42">2022-03-26</time></div></div></div></div></div></div></main><footer id="footer" style="background-image: url('https://img.zsaqwq.com/images/2022/03/26/footer.png')"><div id="footer-wrap"><div class="copyright">&copy;2022 By TideDra</div><div class="footer_custom_text">Hi, welcome to my <a target="_blank" rel="noopener" href="https://www.tidedra.top/">blog</a>!</div></div></footer></div><div id="rightside"><div id="rightside-config-hide"><button id="readmode" type="button" title="阅读模式"><i class="fas fa-book-open"></i></button><button id="darkmode" type="button" title="浅色和深色模式转换"><i class="fas fa-adjust"></i></button><button id="hide-aside-btn" type="button" title="单栏和双栏切换"><i class="fas fa-arrows-alt-h"></i></button></div><div id="rightside-config-show"><button id="rightside_config" type="button" title="设置"><i class="fas fa-cog fa-spin"></i></button><button class="close" id="mobile-toc-button" type="button" title="目录"><i class="fas fa-list-ul"></i></button><a id="to_comment" href="#post-comment" title="直达评论"><i class="fas fa-comments"></i></a><button id="go-up" type="button" title="回到顶部"><i class="fas fa-arrow-up"></i></button></div></div><div id="algolia-search"><div class="search-dialog"><nav class="search-nav"><span class="search-dialog-title">Algolia</span><button class="search-close-button"><i class="fas fa-times"></i></button></nav><div class="search-wrap"><div id="algolia-search-input"></div><hr/><div id="algolia-search-results"><div id="algolia-hits"></div><div id="algolia-pagination"></div><div id="algolia-stats"></div></div></div></div><div id="search-mask"></div></div><div><script src="/js/utils.js"></script><script src="/js/main.js"></script><script src="https://cdn.jsdelivr.net/npm/@fancyapps/ui/dist/fancybox.umd.js"></script><script src="https://cdn.jsdelivr.net/npm/instant.page/instantpage.min.js" type="module"></script><script src="https://cdn.jsdelivr.net/npm/node-snackbar/dist/snackbar.min.js"></script><script src="https://cdn.jsdelivr.net/npm/algoliasearch@4/dist/algoliasearch-lite.umd.js"></script><script src="https://cdn.jsdelivr.net/npm/instantsearch.js@4/dist/instantsearch.production.min.js"></script><script src="/js/search/algolia.js"></script><script>var preloader = {
  endLoading: () => {
    document.body.style.overflow = 'auto';
    document.getElementById('loading-box').classList.add("loaded")
  },
  initLoading: () => {
    document.body.style.overflow = '';
    document.getElementById('loading-box').classList.remove("loaded")

  }
}
window.addEventListener('load',preloader.endLoading())</script><div class="js-pjax"><link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/npm/katex@latest/dist/katex.min.css"><script src="https://cdn.jsdelivr.net/npm/katex@latest/dist/contrib/copy-tex.min.js"></script><link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/npm/katex@latest/dist/contrib/copy-tex.css"><script>(() => {
  document.querySelectorAll('#article-container span.katex-display').forEach(item => {
    btf.wrap(item, 'div', { class: 'katex-wrap'})
  })
})()</script><script>function loadValine () {
  function initValine () {
    const valine = new Valine(Object.assign({
      el: '#vcomment',
      appId: 'Ma2oIbe5SQqQadapIhXlTNs3-MdYXbMMI',
      appKey: 'QEjKMYAuQqGLtfegLWHkhles',
      avatar: 'monsterid',
      serverURLs: '',
      emojiMaps: "",
      path: window.location.pathname,
      visitor: true
    }, {"meta":["nick","mail"],"requiredFields":["nick","mail"],"serverURLs":"https://ma2oibe5.api.lncldglobal.com"}))
  }

  if (typeof Valine === 'function') initValine() 
  else getScript('https://cdn.jsdelivr.net/npm/valine/dist/Valine.min.js').then(initValine)
}

if ('Valine' === 'Valine' || !false) {
  if (false) btf.loadComment(document.getElementById('vcomment'),loadValine)
  else setTimeout(loadValine, 0)
} else {
  function loadOtherComment () {
    loadValine()
  }
}</script></div><script src="https://cdn.jsdelivr.net/npm/pjax/pjax.min.js"></script><script>let pjaxSelectors = ["title","#config-diff","#body-wrap","#rightside-config-hide","#rightside-config-show",".js-pjax"]

var pjax = new Pjax({
  elements: 'a:not([target="_blank"])',
  selectors: pjaxSelectors,
  cacheBust: false,
  analytics: false,
  scrollRestoration: false
})

document.addEventListener('pjax:send', function () {

  // removeEventListener scroll 
  window.tocScrollFn && window.removeEventListener('scroll', window.tocScrollFn)
  window.scrollCollect && window.removeEventListener('scroll', scrollCollect)

  typeof preloader === 'object' && preloader.initLoading()
  document.getElementById('rightside').style.cssText = "opacity: ''; transform: ''"
  
  if (window.aplayers) {
    for (let i = 0; i < window.aplayers.length; i++) {
      if (!window.aplayers[i].options.fixed) {
        window.aplayers[i].destroy()
      }
    }
  }

  typeof typed === 'object' && typed.destroy()

  //reset readmode
  const $bodyClassList = document.body.classList
  $bodyClassList.contains('read-mode') && $bodyClassList.remove('read-mode')

})

document.addEventListener('pjax:complete', function () {
  window.refreshFn()

  document.querySelectorAll('script[data-pjax]').forEach(item => {
    const newScript = document.createElement('script')
    const content = item.text || item.textContent || item.innerHTML || ""
    Array.from(item.attributes).forEach(attr => newScript.setAttribute(attr.name, attr.value))
    newScript.appendChild(document.createTextNode(content))
    item.parentNode.replaceChild(newScript, item)
  })

  GLOBAL_CONFIG.islazyload && window.lazyLoadInstance.update()

  typeof chatBtnFn === 'function' && chatBtnFn()
  typeof panguInit === 'function' && panguInit()

  // google analytics
  typeof gtag === 'function' && gtag('config', '', {'page_path': window.location.pathname});

  // baidu analytics
  typeof _hmt === 'object' && _hmt.push(['_trackPageview',window.location.pathname]);

  typeof loadMeting === 'function' && document.getElementsByClassName('aplayer').length && loadMeting()

  // prismjs
  typeof Prism === 'object' && Prism.highlightAll()

  typeof preloader === 'object' && preloader.endLoading()
})

document.addEventListener('pjax:error', (e) => {
  if (e.request.status === 404) {
    pjax.loadUrl('/404.html')
  }
})</script><script async data-pjax src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script></div></body></html>